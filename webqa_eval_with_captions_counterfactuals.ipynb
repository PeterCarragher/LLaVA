{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_1022 import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "qids = [\n",
    "    \"d5c745d00dba11ecb1e81171463288e9\",\n",
    "    \"d5c0f39c0dba11ecb1e81171463288e9\",\n",
    "    \"d5c164b20dba11ecb1e81171463288e9\",\n",
    "    \"d5cbb1880dba11ecb1e81171463288e9\",\n",
    "    \"d5c747c40dba11ecb1e81171463288e9\",\n",
    "    \"d5c312260dba11ecb1e81171463288e9\",\n",
    "    \"d5c3fdbc0dba11ecb1e81171463288e9\",\n",
    "    \"d5c93d400dba11ecb1e81171463288e9\",\n",
    "    \"d5c506940dba11ecb1e81171463288e9\",\n",
    "    \"d5cacb920dba11ecb1e81171463288e9\",\n",
    "    \"d5ca118e0dba11ecb1e81171463288e9\",\n",
    "    \"d5c2f8e00dba11ecb1e81171463288e9\",\n",
    "    \"d5c944fc0dba11ecb1e81171463288e9\"\n",
    "]\n",
    "\n",
    "\n",
    "exp_name = \"webqa_captions_counterfactual\"\n",
    "data_dir = \"/home/pcarragh/dev/webqa/MultiModalQA/data/\"\n",
    "data_dir += \"WebQA_train_val_obj_v2.json\"\n",
    "\n",
    "with open(data_dir, 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "# filter by qids\n",
    "eval_data = {k: v for k, v in eval_data.items() if k in qids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 104857.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5c0f39c0dba11ecb1e81171463288e9 \"Is the Sumatran orangutan's skin on it's face as smooth as the skin on the Theropithecus gelada's face?\"\n",
      "d5c312260dba11ecb1e81171463288e9 \"Is there a large tower that lights up at night in Las Vegas?\"\n",
      "d5c3fdbc0dba11ecb1e81171463288e9 \"Do the soccer balls used in the FIFA World Cup always have the same pattern?\"\n",
      "d5c506940dba11ecb1e81171463288e9 \"Is the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\"?\"\n",
      "d5c944fc0dba11ecb1e81171463288e9 \"Does SANAE_IV have more large buildings than Neumayer Station?\"\n",
      "d5ca118e0dba11ecb1e81171463288e9 \"Are people restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem?\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "two_image_questions = {k:eval_data[k]['Q'] for k in tqdm(list(eval_data.keys())) if len(eval_data[k][\"img_posFacts\"]) == 2}\n",
    "for k in two_image_questions:\n",
    "    if not 'both' in two_image_questions[k]:\n",
    "        print(k, two_image_questions[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-22 17:24:15,091] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model, eval\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "disable_torch_init()\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name\n",
    ")\n",
    "\n",
    "def llava_eval_on_webqa_sample(question, image_files):\n",
    "    args = type('Args', (), {\n",
    "        \"model_path\": model_path,\n",
    "        \"model_base\": None,\n",
    "        \"model_name\": model_name,\n",
    "        \"query\": question,\n",
    "        \"conv_mode\": None,\n",
    "        \"image_file\": image_files,\n",
    "        \"sep\": \",\",\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": None,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 512,\n",
    "    })()\n",
    "\n",
    "    return eval(tokenizer, model, image_processor,  args)\n",
    "\n",
    "def webqa_accuracy(answer, label, Qcate):\n",
    "    if Qcate == 'color':\n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", color_set)\n",
    "    elif Qcate == 'shape': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", shape_set)\n",
    "    elif Qcate == 'yesno': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", yesno_set)\n",
    "    elif Qcate == 'number': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", {\"NUMBER\"})\n",
    "    else:\n",
    "        return None\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)\n",
    "\n",
    "def accuracy_agg_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in single_image_keys])\n",
    "    two_image_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in two_image_keys])\n",
    "    avr_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items()])\n",
    "    return (single_acc, two_image_acc, avr_acc)\n",
    "\n",
    "def accuracy_agg_generated_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in single_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    two_image_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in two_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    avr_acc = np.mean([PR_avg for key, dict in qa_results.items() for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    \n",
    "    return (single_acc, two_image_acc, avr_acc)\n",
    "\n",
    "def get_prompt(key, reverse_images = False):\n",
    "    imgs = eval_data[key]['img_posFacts']\n",
    "    if len(imgs) == 1:\n",
    "        return f\"<image-placeholder> \\n Caption: {imgs[0]['title']} \\n Question: {eval_data[key]['Q']}\"\n",
    "    assert(len(imgs) == 2)\n",
    "    if reverse_images:\n",
    "        return f\"<image-placeholder> \\n Caption: {imgs[1]['title']} \\n <image-placeholder> \\n Caption: {imgs[0]['title']} \\n Question: {eval_data[key]['Q']}\"\n",
    "    return f\"<image-placeholder> \\n Caption: {imgs[0]['title']} \\n <image-placeholder> \\n Caption: {imgs[1]['title']} \\n Question: {eval_data[key]['Q']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAVA baseline on original images that have perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"Is the Sumatran orangutan's skin on it's face as smooth as the skin on the Theropithecus gelada's face?\", Answer: No, Label: ['\"The Sumatran orangutan\\'s skin on its face is not as as smooth as the skin on the Theropithecus gelada\\'s face.\"']\n",
      "Question: \"is the woman holding something made of paper in both Sleeping Girl and The Love Letter?\", Answer: Yes, the woman in both paintings is holding a piece of paper., Label: ['\"Yes the woman is holding something made of paper in both Sleeping Girl and The Love Letter.\"']\n",
      "Question: \"Are all of the flags near the Broken track on Expedition Everest the same color?\", Answer: No, the flags near the broken track on Expedition Everest are not all the same color. They come in various colors, adding to the visual appeal of the scene., Label: ['\"All of the flags near the Broken track on Expedition Everest are not the same color.\"']\n",
      "Question: \"Is there a large tower that lights up at night in Las Vegas?\", Answer: Yes, there is a large tower that lights up at night in Las Vegas. The tower is the Eiffel Tower, which is a famous landmark in Paris, France. However, in this image, the Eiffel Tower is located in Las Vegas, Nevada, and is part of a hotel and casino complex. The tower is lit up at night, creating a unique and iconic sight in the city., Label: ['\"Yes, there is a large tower that lights up at night in Las Vegas.\"']\n",
      "Question: \"Do the soccer balls used in the FIFA World Cup always have the same pattern?\", Answer: No, the soccer balls used in the FIFA World Cup do not always have the same pattern. The design of the soccer balls changes every four years to reflect the host country's culture and artistic expression. The 2014 FIFA World Cup, held in Brazil, featured a soccer ball with a green and yellow design, while the 2018 FIFA World Cup, held in Russia, featured a soccer ball with a blue and white design., Label: ['\"No, the soccer balls used in the FIFA World Cup do not always have the same pattern.\"']\n",
      "Question: \"Is the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\"?\", Answer: Yes, the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin is larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge.\", Label: ['\"No, the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin is not larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\".\"']\n",
      "Question: \"Are the ears of the Persian Leopard wider than its paws?\", Answer: Yes, Label: ['\"No, its ears are not wider than its paws.\"']\n",
      "Question: \"Do River at Lavacourt and The Beach and the Falaise d'Amont both depict a boat and a body of water?\", Answer: Yes, both River at Lavacourt and The Beach and the Falaise d'Amont depict a boat and a body of water., Label: ['\"River at Lavacourt and The Beach and the Falaise d\\'Amont both depict a boat and a body of water.\"']\n",
      "Question: \"Do the Cotton Top Tamarin and the Northern Muriqui both have tufted crests on their heads?\", Answer: Yes, both the Cotton Top Tamarin and the Northern Muriqui have tufted crests on their heads., Label: ['\"The Cotton Top Tamarin and the Northern Muriqui don\\'t both have tufted crests on their heads.\"']\n",
      "Question: \"Does SANAE_IV have more large buildings than Neumayer Station?\", Answer: No, Label: ['\"Yes SANAE IV has three large buildings, while Neumayer Station has only one.\"']\n",
      "Question: \"Are people restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem?\", Answer: Yes, people are restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem. The image shows a group of people gathered around the altar, but they are not allowed to touch it. This is because the altar is considered sacred and is meant to be respected and admired from a distance. The people in the image are likely observing the altar and the religious artifacts on display, such as the cross and the candles, without touching them., Label: ['\"No, people are not restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem.\"']\n",
      "Question: \"Are there only  rectangular and arched windows that can be found inside the Louvre at Cour Puget in the Richelieu Wing?\", Answer: No, Label: ['\"No, there are also square windows inside the Louvre at Cour Puget in the Richelieu Wing.\"']\n",
      "Question: \"Is the top side of the cap of the Leccinum Scabrum mushroom the same color as its stalk?\", Answer: Yes, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "(0.25, 0.3333333333333333, 0.3076923076923077)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "llava_results_baseline = {}\n",
    "for k in list(eval_data.keys()):\n",
    "    question = get_prompt(k)\n",
    "    # print(question)\n",
    "    image_files = ','.join([str(img_data['image_id']) for img_data in eval_data[k]['img_posFacts']])\n",
    "    # try:\n",
    "    answer = llava_eval_on_webqa_sample(question, image_files)\n",
    "    # except:\n",
    "    #     answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_baseline[k] = webqa_accuracy(answer, label, Qcate)\n",
    "    \n",
    "    print(f\"Question: {eval_data[k]['Q']}, Answer: {answer}, Label: {label}\")\n",
    "\n",
    "print(accuracy_agg_results(llava_results_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"Is the Sumatran orangutan's skin on it's face as smooth as the skin on the Theropithecus gelada's face?\", Answer: No, Label: ['\"The Sumatran orangutan\\'s skin on its face is not as as smooth as the skin on the Theropithecus gelada\\'s face.\"']\n",
      "Question: \"is the woman holding something made of paper in both Sleeping Girl and The Love Letter?\", Answer: Yes, in both Sleeping Girl and The Love Letter, the woman is holding a piece of paper., Label: ['\"Yes the woman is holding something made of paper in both Sleeping Girl and The Love Letter.\"']\n",
      "Question: \"Are all of the flags near the Broken track on Expedition Everest the same color?\", Answer: Yes, Label: ['\"All of the flags near the Broken track on Expedition Everest are not the same color.\"']\n",
      "Question: \"Is there a large tower that lights up at night in Las Vegas?\", Answer: Yes, Label: ['\"Yes, there is a large tower that lights up at night in Las Vegas.\"']\n",
      "Question: \"Do the soccer balls used in the FIFA World Cup always have the same pattern?\", Answer: No, the soccer balls used in the FIFA World Cup do not always have the same pattern. The design of the soccer ball can vary from one tournament to another, and even within the same tournament, different balls may be used for different matches. The choice of the soccer ball design is usually made to ensure that the ball meets the specific requirements of the tournament, such as providing a good balance between performance and safety., Label: ['\"No, the soccer balls used in the FIFA World Cup do not always have the same pattern.\"']\n",
      "Question: \"Is the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\"?\", Answer: Yes, the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin is larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\"., Label: ['\"No, the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin is not larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\".\"']\n",
      "Question: \"Are the ears of the Persian Leopard wider than its paws?\", Answer: Yes, Label: ['\"No, its ears are not wider than its paws.\"']\n",
      "Question: \"Do River at Lavacourt and The Beach and the Falaise d'Amont both depict a boat and a body of water?\", Answer: No, River at Lavacourt and The Beach and the Falaise d'Amont do not both depict a boat and a body of water. River at Lavacourt is a painting of a river, while The Beach and the Falaise d'Amont is a painting of a beach and a cliff., Label: ['\"River at Lavacourt and The Beach and the Falaise d\\'Amont both depict a boat and a body of water.\"']\n",
      "Question: \"Do the Cotton Top Tamarin and the Northern Muriqui both have tufted crests on their heads?\", Answer: Yes, both the Cotton Top Tamarin and the Northern Muriqui have tufted crests on their heads., Label: ['\"The Cotton Top Tamarin and the Northern Muriqui don\\'t both have tufted crests on their heads.\"']\n",
      "Question: \"Does SANAE_IV have more large buildings than Neumayer Station?\", Answer: No, Label: ['\"Yes SANAE IV has three large buildings, while Neumayer Station has only one.\"']\n",
      "Question: \"Are people restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem?\", Answer: Yes, people are restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem. The altar is considered sacred and is not to be touched by anyone, including the priests. This is to maintain the purity and sanctity of the holy site., Label: ['\"No, people are not restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem.\"']\n",
      "Question: \"Are there only  rectangular and arched windows that can be found inside the Louvre at Cour Puget in the Richelieu Wing?\", Answer: No, Label: ['\"No, there are also square windows inside the Louvre at Cour Puget in the Richelieu Wing.\"']\n",
      "Question: \"Is the top side of the cap of the Leccinum Scabrum mushroom the same color as its stalk?\", Answer: Yes, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "(0.25, 0.3333333333333333, 0.3076923076923077)\n"
     ]
    }
   ],
   "source": [
    "llava_results_blank = {}\n",
    "for k in list(eval_data.keys()):\n",
    "    question = get_prompt(k)\n",
    "    image_files ='playground/counterfactual_exp/BLANK.jpg,playground/counterfactual_exp/BLANK.jpg'\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(question, image_files)\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava_blank'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_blank[k] = webqa_accuracy(answer, label, Qcate)\n",
    "    print(f\"Question: {eval_data[k]['Q']}, Answer: {answer}, Label: {label}\")\n",
    "    \n",
    "print(accuracy_agg_results(llava_results_blank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"Is the Sumatran orangutan's skin on it's face as smooth as the skin on the Theropithecus gelada's face?\", Answer: No, the Sumatran orangutan's skin on its face is not as smooth as the Theropithecus gelada's face. The Theropithecus gelada has a smoother, more hairless face, while the Sumatran orangutan has a more hairy and rougher face., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"is the woman holding something made of paper in both Sleeping Girl and The Love Letter?\", Answer: Yes, in both paintings, the woman is holding a piece of paper in her hand., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Are all of the flags near the Broken track on Expedition Everest the same color?\", Answer: No, not all of the flags near the Broken track on Expedition Everest are the same color., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Is there a large tower that lights up at night in Las Vegas?\", Answer: Yes, there is a large tower that lights up at night in Las Vegas. The image shows a night view of the Las Vegas Strip, which is a famous area in the city known for its bright lights and iconic landmarks. The tower is part of the famous Las Vegas hotels, such as the Bellagio, which is known for its fountain show and the iconic \"O\" show. The tower's lights create a vibrant and lively atmosphere, attracting tourists and visitors to the city., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Do the soccer balls used in the FIFA World Cup always have the same pattern?\", Answer: No, the soccer balls used in the FIFA World Cup do not always have the same pattern. The image shows two soccer players on a field, and one of them is kicking a soccer ball. The ball has a distinctive pattern, which is different from the one used in the previous match. The FIFA World Cup uses a specific type of soccer ball, which is designed to provide optimal performance and visibility on the field. The ball's design may vary from one tournament to another, and it may also be different from the standard soccer ball used in regular matches., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Is the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge\"?\", Answer: Yes, the bridge in the \"Italian Landscape\" painting by Jean-Victor Bertin is larger than the bridge in the background of the painting \"The Opening of Waterloo Bridge.\", Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Are the ears of the Persian Leopard wider than its paws?\", Answer: Yes, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Do River at Lavacourt and The Beach and the Falaise d'Amont both depict a boat and a body of water?\", Answer: Yes, both River at Lavacourt and The Beach and the Falaise d'Amont depict a boat and a body of water., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Do the Cotton Top Tamarin and the Northern Muriqui both have tufted crests on their heads?\", Answer: Yes, both the Cotton Top Tamarin and the Northern Muriqui have tufted crests on their heads., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Does SANAE_IV have more large buildings than Neumayer Station?\", Answer: No, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Are people restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem?\", Answer: Yes, people are restricted from touching the altar in the Church of the Holy Sepulchre in Jerusalem. The altar is a sacred and holy place, and it is meant to be respected and preserved. The image shows a man and a woman standing in front of the altar, but they are not allowed to touch it. The presence of the man and woman in the image suggests that they are tourists or visitors, and they are likely observing the altar from a distance, as is the custom in such religious sites., Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Are there only  rectangular and arched windows that can be found inside the Louvre at Cour Puget in the Richelieu Wing?\", Answer: No, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "Question: \"Is the top side of the cap of the Leccinum Scabrum mushroom the same color as its stalk?\", Answer: Yes, Label: ['\"The top side of the Leccinum Scabrum mushroom\\'s cap is not the same color as the stem.\"']\n",
      "(0.25, 0.3333333333333333, 0.3076923076923077)\n",
      "(nan, nan, nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label = {}\n",
    "llava_results_perturbed_generated_label = {}\n",
    "\n",
    "perturbation_path = \"/home/pcarragh/dev/webqa/image_gen_val/val_images_perturbed_gpt_obj_lama\"\n",
    "\n",
    "for k in list(eval_data.keys()):\n",
    "    eval_data[k]['A_perturbed_llava'] = {}\n",
    "    question = get_prompt(k)\n",
    "    image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "    \n",
    "    # image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "    image_files = \"\"\n",
    "    for img in eval_data[k]['img_posFacts']:\n",
    "        generated_path = f\"{perturbation_path}/{str(image_id)}_{k}.jpeg\"\n",
    "        if os.path.exists(generated_path):\n",
    "            image_files += \",\" + generated_path\n",
    "        else:\n",
    "            image_files += \",\" + str(img['image_id'])\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(\"Answer the following question based on the provided images. \\n\" + question, image_files[1:])\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "\n",
    "    original_label = eval_data[k]['A']\n",
    "    eval_data[k]['A_perturbed_llava'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    print(f\"Question: {eval_data[k]['Q']}, Answer: {answer}, Label: {label}\")\n",
    "    llava_results_perturbed_original_label[k] = webqa_accuracy(answer, original_label, Qcate)\n",
    "\n",
    "print(accuracy_agg_results(llava_results_perturbed_original_label))\n",
    "print(accuracy_agg_results(llava_results_perturbed_generated_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 572/572 [10:54<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.12626262626262627, 0.35291164658634533, 0.17006208769887463)\n",
      "(0.7344877344877345, 0.5276104417670683, 0.6945091191307723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label_reversed = {}\n",
    "llava_results_perturbed_generated_label_reversed = {}\n",
    "\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    llava_results_perturbed_original_label_reversed[k] = {}\n",
    "    llava_results_perturbed_generated_label_reversed[k] = {}\n",
    "    eval_data[k]['A_perturbed_llava_reversed'] = {}\n",
    "    question = get_prompt(k, True)\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "        image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "        if len(eval_data[k]['img_posFacts']) > 1:\n",
    "            image_2 = eval_data[k]['img_posFacts'][1]['image_id']\n",
    "            image_files = str(image_2) + ',' + image_files\n",
    "        try:\n",
    "            answer = llava_eval_on_webqa_sample(question, image_files)\n",
    "        except:\n",
    "            answer = 'ERROR: llava failed'\n",
    "\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llava_reversed'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llava_results_perturbed_original_label_reversed[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llava_results_perturbed_generated_label_reversed[k][idx] = webqa_accuracy(answer, [label], Qcate)\n",
    "\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_original_label_reversed))\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_generated_label_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 572/572 [02:55<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, 0.17369477911646586, 0.17369477911646586)\n",
      "(nan, 0.6912650602409639, 0.6912650602409639)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label_double_gen = {}\n",
    "llava_results_perturbed_generated_label_double_gen = {}\n",
    "\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    llava_results_perturbed_original_label_double_gen[k] = {}\n",
    "    llava_results_perturbed_generated_label_double_gen[k] = {}\n",
    "    eval_data[k]['A_perturbed_llava_reversed'] = {}\n",
    "    question = get_prompt(k, True)\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "        image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "        if len(eval_data[k]['img_posFacts']) > 1:\n",
    "            # image_2 = eval_data[k]['img_posFacts'][1]['image_id']\n",
    "            image_files = image_files + ',' + image_files\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            answer = llava_eval_on_webqa_sample(question, image_files)\n",
    "        except:\n",
    "            answer = 'ERROR: llava failed'\n",
    "\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llava_reversed'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llava_results_perturbed_original_label_double_gen[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llava_results_perturbed_generated_label_double_gen[k][idx] = webqa_accuracy(answer, [label], Qcate)\n",
    "\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_original_label_double_gen))\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_generated_label_double_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 572/572 [02:49<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, 0.7479919678714859, 0.7479919678714859)\n",
      "(nan, 0.05823293172690764, 0.05823293172690764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label_double_orig = {}\n",
    "llava_results_perturbed_generated_label_double_orig = {}\n",
    "\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    llava_results_perturbed_original_label_double_orig[k] = {}\n",
    "    llava_results_perturbed_generated_label_double_orig[k] = {}\n",
    "    eval_data[k]['A_perturbed_llava_reversed'] = {}\n",
    "    question = get_prompt(k, True)\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "        image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "        if len(eval_data[k]['img_posFacts']) > 1:\n",
    "            image_2 = str(eval_data[k]['img_posFacts'][1]['image_id'])\n",
    "            image_files = image_2 + ',' + image_2\n",
    "        else: \n",
    "            continue\n",
    "        try:\n",
    "            answer = llava_eval_on_webqa_sample(question, image_files)\n",
    "        except:\n",
    "            answer = 'ERROR: llava failed'\n",
    "\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llava_reversed'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llava_results_perturbed_original_label_double_orig[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llava_results_perturbed_generated_label_double_orig[k][idx] = webqa_accuracy(answer, [label], Qcate)\n",
    "\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_original_label_double_orig))\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_generated_label_double_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>single_image</th>\n",
       "      <th>two_image</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.723977</td>\n",
       "      <td>0.729885</td>\n",
       "      <td>0.725175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blank</td>\n",
       "      <td>0.298977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perturbed_original_label</td>\n",
       "      <td>0.126263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perturbed_generated_label</td>\n",
       "      <td>0.734488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             experiment name  single_image  two_image   average\n",
       "0                   baseline      0.723977   0.729885  0.725175\n",
       "1                      blank      0.298977   0.000000  0.238345\n",
       "2   perturbed_original_label      0.126263   0.000000  0.101863\n",
       "3  perturbed_generated_label      0.734488   0.000000  0.592549"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_accs = accuracy_agg_results(llava_results_baseline)\n",
    "blank_accs = accuracy_agg_results(llava_results_blank)\n",
    "perturbed_original_label_accs = accuracy_agg_generated_results(llava_results_perturbed_original_label)\n",
    "perturbed_generated_label_acc = accuracy_agg_generated_results(llava_results_perturbed_generated_label)\n",
    "\n",
    "columns = ['experiment name', 'single_image', 'two_image', 'average']\n",
    "accuracy_agg_df = pd.DataFrame(columns=columns)\n",
    "accuracy_agg_df['experiment name'] = ['baseline', 'blank', 'perturbed_original_label', 'perturbed_generated_label']\n",
    "accuracy_agg_df['single_image'] = [baseline_accs[0], blank_accs[0], perturbed_original_label_accs[0], perturbed_generated_label_acc[0]]\n",
    "accuracy_agg_df['two_image'] = [baseline_accs[1], blank_accs[1], perturbed_original_label_accs[1], perturbed_generated_label_acc[1]]\n",
    "accuracy_agg_df['average'] = [baseline_accs[2], blank_accs[2], perturbed_original_label_accs[2], perturbed_generated_label_acc[2]]\n",
    "accuracy_agg_df.to_csv(\"results/{exp_name}.csv\", index=False)\n",
    "accuracy_agg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image and perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for questions that llava gets right regardless of having a blank image, check accuracy on counterfactual examples\n",
    "perturbation_ignored = {key:idx for key, dict in llava_results_perturbed_original_label.items() for idx, (_,_,_,_,acc) in dict.items() if acc > 0}\n",
    "blank_ignored = set([k for k, (_,_,_,_,acc) in llava_results_blank.items() if acc > 0])\n",
    "parametric_ans_keys = set(perturbation_ignored.keys()).intersection(blank_ignored)\n",
    "len(parametric_ans_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \"What colors appeared on Dirk Nowitzki's uniform on October 9, 2009?\"\n",
      "A: ['\"On October 9, 2009, Dirk Nowitzki\\'s uniform was white, dark blue, light blue, and grey.\"']\n",
      "A_llava: The colors on Dirk Nowitzki's uniform on October 9, 2009, were blue and white.\n",
      "A_llava_blank: White\n",
      "A_perturbed_llava: Blue and white\n",
      "2, A_perturbed: {'1': 'black', '2': 'blue'}, A_perturbed_gpt: {'1': 'black', '2': 'blue'}\n",
      "Q: \"What color is the belly of a Green Tree Frog?\"\n",
      "A: ['\"The belly of a Green Tree Frog is white.\"']\n",
      "A_llava: The belly of a Green Tree Frog is white.\n",
      "A_llava_blank: The belly of a Green Tree Frog is white.\n",
      "A_perturbed_llava: The belly of a Green Tree Frog is white.\n",
      "0, A_perturbed: {'0': 'pink'}, A_perturbed_gpt: {'0': 'pink'}\n",
      "Q: \"What are the colors of the Moscow torch?\"\n",
      "A: ['\"The Moscow torch is red, white, and gold.\"']\n",
      "A_llava: The Moscow torch is gold and white.\n",
      "A_llava_blank: The Moscow torch is white and black.\n",
      "A_perturbed_llava: The Moscow torch is gold and black.\n",
      "3, A_perturbed: {'0': 'gold', '2': 'blue', '3': 'gold'}, A_perturbed_gpt: {'0': 'gold', '2': 'blue', '3': 'gold'}\n",
      "Q: \"What is the color of the hat worn by Carel Fabricius in his self portrait (done on 1645)?\"\n",
      "A: ['\"The color of the hat worn by Carel Fabricius in his self portrait (done on 1645) is black.\"']\n",
      "A_llava: The color of the hat worn by Carel Fabricius in his self portrait is black.\n",
      "A_llava_blank: The color of the hat worn by Carel Fabricius in his self portrait (done on 1645) is black.\n",
      "A_perturbed_llava: The color of the hat worn by Carel Fabricius in his self portrait is black.\n",
      "1, A_perturbed: {'1': 'violet'}, A_perturbed_gpt: {'1': 'violet'}\n",
      "Q: \"What colors are found on the Coast Guard boat with number 25500?\"\n",
      "A: ['\"The Coast Guard boat with number 25500 is in red, white, blue, black, and grey.\"']\n",
      "A_llava: The Coast Guard boat with number 25500 has orange and white colors.\n",
      "A_llava_blank: The Coast Guard boat with number 25500 is white and blue.\n",
      "A_perturbed_llava: The Coast Guard boat with number 25500 is orange and white.\n",
      "0, A_perturbed: {'0': 'orange'}, A_perturbed_gpt: {'0': 'orange'}\n",
      "Q: \"What colors are in the imagery of the logo for SeaWorld San Antonio?\"\n",
      "A: ['\"The imagery on the logo for SeaWorld San Antonio is blue and yellow.\"']\n",
      "A_llava: The imagery of the logo for SeaWorld San Antonio features blue and yellow colors.\n",
      "A_llava_blank: The imagery of the logo for SeaWorld San Antonio features blue and white colors.\n",
      "A_perturbed_llava: The imagery of the logo for SeaWorld San Antonio features yellow and black colors.\n",
      "0, A_perturbed: {'0': 'yellow'}, A_perturbed_gpt: {'0': 'yellow'}\n",
      "Q: \"What colors are Charles Talbot's robe in the painting of him by John Vanderbank?\"\n",
      "A: ['\"The colors of Charles Talbot\\'s robe in the painting of him by John Vanderbank are black and gold.\"']\n",
      "A_llava: Charles Talbot's robe in the painting of him by John Vanderbank is black and gold.\n",
      "A_llava_blank: In the painting of Charles Talbot by John Vanderbank, he is wearing a black robe.\n",
      "A_perturbed_llava: In the painting of Charles Talbot by John Vanderbank, he is wearing a black and white robe.\n",
      "1, A_perturbed: {'1': 'violet'}, A_perturbed_gpt: {'1': 'violet'}\n",
      "Q: \"What color are the dishes that the food is served in at Ohana in Belltown?\"\n",
      "A: ['\"The dishes that the food is served in at Ohana in Belltown are white, black and tan.\"']\n",
      "A_llava: The dishes that the food is served in at Ohana in Belltown are black.\n",
      "A_llava_blank: White\n",
      "A_perturbed_llava: The dishes that the food is served in at Ohana in Belltown are white.\n",
      "1, A_perturbed: {'0': 'red', '1': 'black', '2': 'blue', '3': 'violet'}, A_perturbed_gpt: {'0': 'red', '1': 'black', '2': 'blue', '3': 'violet'}\n",
      "Q: \"What color is the cap, stalk, and gills on a Lactarius evosmus?\"\n",
      "A: ['\"The cap, stalk, and gills on a Lactarius evosmus are white.\"']\n",
      "A_llava: The cap, stalk, and gills on a Lactarius evosmus are white.\n",
      "A_llava_blank: The cap, stalk, and gills on a Lactarius evosmus are white.\n",
      "A_perturbed_llava: The cap of a Lactarius evosmus is yellow, the stalk is white, and the gills are brown.\n",
      "0, A_perturbed: {'0': 'yellow', '2': 'yellow', '3': 'yellow'}, A_perturbed_gpt: {'0': 'yellow', '2': 'yellow', '3': 'yellow'}\n",
      "Q: \"What colors are seen from the Fountain's statues in the Parc de Versailles ?\"\n",
      "A: ['\"The colors seen from the Fountain\\'s statues in the Parc de Versailles are white and gold.\"']\n",
      "A_llava: The statues in the Parc de Versailles are gold and brown.\n",
      "A_llava_blank: The colors seen from the Fountain's statues in the Parc de Versailles are white and black.\n",
      "A_perturbed_llava: The colors seen from the Fountain's statues in the Parc de Versailles are pink and gold.\n",
      "3, A_perturbed: {'0': 'golden', '1': 'golden', '3': 'pink'}, A_perturbed_gpt: {'0': 'golden', '1': 'golden', '3': 'pink'}\n",
      "Q: \"Which colors was Chinese gymnast Li Ning wearing when delivering the Olympic torch at the 2008 Summer Olympic Games opening ceremony?\"\n",
      "A: ['\"Chinese gymnast Li Ning was wearing red, white, and grey when delivering the Olympic torch at the 2008 Summer Olympic Games opening ceremony.\"']\n",
      "A_llava: Li Ning was wearing a red and white outfit when delivering the Olympic torch at the 2008 Summer Olympic Games opening ceremony.\n",
      "A_llava_blank: Li Ling was wearing red and white colors when delivering the Olympic torch at the 2008 Summer Olympic Games opening ceremony.\n",
      "A_perturbed_llava: Li Ning was wearing a yellow and red outfit when delivering the Olympic torch at the 2008 Summer Olympic Games opening ceremony.\n",
      "2, A_perturbed: {'2': 'yellow'}, A_perturbed_gpt: {'2': 'yellow'}\n",
      "Q: \"What color is the mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba?\"\n",
      "A: ['\"The mask on the external wall mural at Rainbow Stage is yellow.\"']\n",
      "A_llava: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba is black.\n",
      "A_llava_blank: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba is white.\n",
      "A_perturbed_llava: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba is gold.\n",
      "3, A_perturbed: {'2': 'pink', '3': 'golden'}, A_perturbed_gpt: {'2': 'pink', '3': 'golden'}\n",
      "Q: \"What color pants did the silver medalist at the Taekwondo +80kg awarding ceremony at the 2016 Summer Olympics wear?\"\n",
      "A: ['\"The silver medalist at the Taekwondo +80kg awarding ceremony at the 2016 Summer Olympics wore white pants.\"']\n",
      "A_llava: The silver medalist at the Taekwondo +80kg awarding ceremony at the 2016 Summer Olympics wore blue pants.\n",
      "A_llava_blank: The silver medalist at the Taekwondo +80kg awarding ceremony at the 2016 Summer Olympics wore black pants.\n",
      "A_perturbed_llava: The silver medalist at the Taekwondo +80kg awarding ceremony at the 2016 Summer Olympics wore gold pants.\n",
      "3, A_perturbed: {'1': 'brown', '2': 'white', '3': 'gold'}, A_perturbed_gpt: {'1': 'brown', '2': 'white', '3': 'gold'}\n",
      "Q: \"What color on the platform at 8th Avenue Sea Beach station warns passengers where the edge of the platform ends?\"\n",
      "A: ['\"The color yellow is used on the platform at 8th Avenue Sea Beach station to warn passengers where the edge of the platform ends.\"']\n",
      "A_llava: Yellow\n",
      "A_llava_blank: Yellow\n",
      "A_perturbed_llava: Yellow\n",
      "1, A_perturbed: {'1': 'red'}, A_perturbed_gpt: {'1': 'red'}\n",
      "Q: \"Which colors illuminated the olympic stadium during the 2016 Summer Olympics opening ceremony?\"\n",
      "A: ['\"The colors illuminated the Olympic stadium during the 2016 Summer Olympics opening ceremony were red, purple green and yellow.\"']\n",
      "A_llava: The Olympic stadium was illuminated with red, white, and blue colors during the 2016 Summer Olympics opening ceremony.\n",
      "A_llava_blank: The Olympic stadium was illuminated with red, white, and blue colors during the 2016 Summer Olympics opening ceremony.\n",
      "A_perturbed_llava: The Olympic stadium was illuminated with purple and pink colors during the 2016 Summer Olympics opening ceremony.\n",
      "3, A_perturbed: {'3': 'pink'}, A_perturbed_gpt: {'3': 'pink'}\n",
      "Q: \"What color is the sign on the Mayo Civic Center?\"\n",
      "A: ['\"The sign on the Mayo Civic Center is white.\"']\n",
      "A_llava: The sign on the Mayo Civic Center is red.\n",
      "A_llava_blank: The sign on the Mayo Civic Center is white.\n",
      "A_perturbed_llava: The sign on the Mayo Civic Center is white.\n",
      "2, A_perturbed: {'2': 'gray'}, A_perturbed_gpt: {'2': 'gray'}\n",
      "Q: \"What color are the walls at The Blue Hour Contemporary Art Gallery in Vancouver?\"\n",
      "A: ['\"The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\"']\n",
      "A_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_llava_blank: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_perturbed_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are black.\n",
      "2, A_perturbed: {'2': 'gray'}, A_perturbed_gpt: {'2': 'gray'}\n",
      "Q: \"What color jacket is Mike Tyson wearing at the Heavyweight Exhibit at Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada?\"\n",
      "A: ['\"Mike Tyson wore a black jacket at the Heavyweight Exhibit at the Boxing Hall of Fame in Las Vegas, Nevada.\"']\n",
      "A_llava: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "A_llava_blank: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "A_perturbed_llava: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "3, A_perturbed: {'1': 'red', '2': 'brown', '3': 'blue'}, A_perturbed_gpt: {'1': 'red', '2': 'brown', '3': 'blue'}\n",
      "Q: \"What color is the backdrop behind the golden gun displayed at the International Spy Museum?\"\n",
      "A: ['\"The backdrop behind the golden gun displayed at the International Spy Museum is red.\"']\n",
      "A_llava: The backdrop behind the golden gun displayed at the International Spy Museum is red.\n",
      "A_llava_blank: The backdrop behind the golden gun displayed at the International Spy Museum is white.\n",
      "A_perturbed_llava: The backdrop behind the golden gun displayed at the International Spy Museum is green.\n",
      "1, A_perturbed: {'1': 'teal'}, A_perturbed_gpt: {'1': 'teal'}\n",
      "Q: \"What color was the beanie worn by the winner of the silver medal for Freestyle skiing at the 2018 Winter Olympics?\"\n",
      "A: ['\"The beanie worn by the winner of the silver medal for Freestyle skiing at the 2018 Winter Olympics was white.\"']\n",
      "A_llava: The winner of the silver medal for Freestyle skiing at the 2018 Winter Olympics wore a red beanie.\n",
      "A_llava_blank: The beanie worn by the winner of the silver medal for Freestyle skiing at the 2018 Winter Olympics was black.\n",
      "A_perturbed_llava: The winner of the silver medal for Freestyle skiing at the 2018 Winter Olympics wore a red beanie.\n",
      "2, A_perturbed: {'1': 'blue', '2': 'red'}, A_perturbed_gpt: {'1': 'blue', '2': 'red'}\n",
      "Q: \"What color shirt can be worn as part of the uniform in a classroom in Niue?\"\n",
      "A: ['\"White or grey shirts are part of the uniform.\"']\n",
      "A_llava: White\n",
      "A_llava_blank: White\n",
      "A_perturbed_llava: White\n",
      "1, A_perturbed: {'1': 'red'}, A_perturbed_gpt: {'1': 'red'}\n",
      "Q: \"What colored stripe do the Caballo Haflinge horses have on their head?\"\n",
      "A: ['\"The stripe is white.\"']\n",
      "A_llava: The Caballo Haflinge horses have a white stripe on their head.\n",
      "A_llava_blank: The Caballo Haflinge horses have a white stripe on their head.\n",
      "A_perturbed_llava: The Caballo Haflinge horses have a white stripe on their head.\n",
      "3, A_perturbed: {'3': 'orange'}, A_perturbed_gpt: {'3': 'orange'}\n",
      "Q: \"What color is the text below the white paint splash in the street art war graffiti by Jacek Tylicki?\"\n",
      "A: ['\"The color is the text below the white paint splash in the street art war graffiti by Jacek Tylicki is white.\"']\n",
      "A_llava: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is black.\n",
      "A_llava_blank: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is black.\n",
      "A_perturbed_llava: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is yellow.\n",
      "1, A_perturbed: {'1': 'yellow'}, A_perturbed_gpt: {'1': 'yellow'}\n",
      "Q: \"What colored spots does the Dinocampus_coccinellae_cocon have on its shell?\"\n",
      "A: ['\"The Dinocampus coccinellae cocon has black spots on its shell.\"']\n",
      "A_llava: The Dinocampus coccinellae cocon has red spots on its shell.\n",
      "A_llava_blank: The Dinocampus coccinellae cocon has black spots on its shell.\n",
      "A_perturbed_llava: The Dinocampus coccinellae cocon has purple spots on its shell.\n",
      "0, A_perturbed: {'0': 'purple'}, A_perturbed_gpt: {'0': 'purple'}\n",
      "Q: \"What color is the border around Malevich's Black Square?\"\n",
      "A: ['\"The border around Malevich\\'s Black Square is white.\"']\n",
      "A_llava: The border around Malevich's Black Square is white.\n",
      "A_llava_blank: The border around Malevich's Black Square is black.\n",
      "A_perturbed_llava: The border around Malevich's Black Square is gold.\n",
      "3, A_perturbed: {'1': 'pink', '3': 'golden'}, A_perturbed_gpt: {'1': 'pink', '3': 'gold'}\n",
      "Q: \"What color is the sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK?\"\n",
      "A: ['\"The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK is white.\"']\n",
      "A_llava: The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK is white.\n",
      "A_llava_blank: The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK is white.\n",
      "A_perturbed_llava: The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK is white.\n",
      "2, A_perturbed: {'2': 'gray'}, A_perturbed_gpt: {'2': 'gray'}\n",
      "Q: \"What colors of flames are emitted from the torch on the 1980 USSR Olympic mascot stamp?\"\n",
      "A: ['\"On the 1980 USSR Olympic mascot stamp, the color of the flames emitted from the torch are red, orange, yellow, green, and blue.\"']\n",
      "A_llava: The torch on the 1980 USSR Olympic mascot stamp emits red, white, and blue flames.\n",
      "A_llava_blank: The 1980 USSR Olympic mascot stamp features a torch with blue and red flames.\n",
      "A_perturbed_llava: The flames emitted from the torch on the 1980 USSR Olympic mascot stamp are orange and black.\n",
      "0, A_perturbed: {'0': 'orange'}, A_perturbed_gpt: {'0': 'orange'}\n",
      "Q: \"What colors are the sculptures in the rotunda of the Altes Museum in Berlin?\"\n",
      "A: ['\"Most of the sculptures in the rotunda of the Altes Museum in Berlin are white with some brown discoloration.\"']\n",
      "A_llava: The sculptures in the rotunda of the Altes Museum in Berlin are white and red.\n",
      "A_llava_blank: The sculptures in the rotunda of the Altes Museum in Berlin are black and white.\n",
      "A_perturbed_llava: The sculptures in the rotunda of the Altes Museum in Berlin are pink and white.\n",
      "3, A_perturbed: {'2': 'gold', '3': 'pink'}, A_perturbed_gpt: {'2': 'gold', '3': 'pink'}\n",
      "Q: \"What color spots do Paphiopedilum acmodontum have?\"\n",
      "A: ['\"Paphiopedilum acmodontum have dark brown colored spots.\"']\n",
      "A_llava: Paphiopedilum acmodontum have pink spots.\n",
      "A_llava_blank: Paphiopedilum acmodontum have white spots.\n",
      "A_perturbed_llava: Paphiopedilum acmodontum have blue spots.\n",
      "2, A_perturbed: {'0': 'purple', '1': 'brown', '2': 'blue'}, A_perturbed_gpt: {'0': 'purple', '1': 'brown', '2': 'blue'}\n",
      "Q: \"What colors are the uniforms for the torch runners at the 2012 Olympics?\"\n",
      "A: ['\"The colors of the uniforms for the torch runners at the 2012 Olympics are White and Gold.\"']\n",
      "A_llava: The uniforms for the torch runners at the 2012 Olympics are white and grey.\n",
      "A_llava_blank: The uniforms for the torch runners at the 2012 Olympics are white and blue.\n",
      "A_perturbed_llava: The uniforms for the torch runners at the 2012 Olympics are pink and white.\n",
      "1, A_perturbed: {'1': 'pink'}, A_perturbed_gpt: {'1': 'pink'}\n",
      "Q: \"What colors can be found on the kiosk coverings on Khao San road?\"\n",
      "A: ['\"Yellow, green, blue, red, and white can be found on the kiosk coverings on Khao San road.\"']\n",
      "A_llava: The kiosk coverings on Khao San road are yellow and pink.\n",
      "A_llava_blank: The kiosk coverings on Khao San road are white and black.\n",
      "A_perturbed_llava: The kiosk coverings on Khao San road are yellow and orange.\n",
      "3, A_perturbed: {'3': 'yellow'}, A_perturbed_gpt: {'3': 'yellow'}\n",
      "Q: \"What color are the spots on a Capepetrel kinggeorgeisland?\"\n",
      "A: ['\"The spots on a Capepetrel kinggeorgeisland are black.\"']\n",
      "A_llava: The spots on a Capepetrel kinggeorgeisland are white.\n",
      "A_llava_blank: The spots on a Capepetrel kinggeorgeisland are black.\n",
      "A_perturbed_llava: The spots on a Capepetrel kinggeorgeisland are yellow.\n",
      "2, A_perturbed: {'2': 'yellow'}, A_perturbed_gpt: {'2': 'yellow'}\n",
      "Q: \"What color is the Scholastic building in New York?\"\n",
      "A: ['\"The Scholastic building in New York is white and red.\"']\n",
      "A_llava: The Scholastic building in New York is green.\n",
      "A_llava_blank: The Scholastic building in New York is white.\n",
      "A_perturbed_llava: The Scholastic building in New York is white.\n",
      "0, A_perturbed: {'0': 'white', '1': 'gold'}, A_perturbed_gpt: {'0': 'white', '1': 'gold'}\n",
      "Q: \"What colors are present on the face of the 2010 Winter Olympics mascot, Miga?\"\n",
      "A: ['\"White, black, and pink are present on the face of the 2010 Winter Olympics mascot, Miga.\"']\n",
      "A_llava: The face of the 2010 Winter Olympics mascot, Miga, has pink and white colors.\n",
      "A_llava_blank: The face of the 2010 Winter Olympics mascot, Miga, has black and white colors.\n",
      "A_perturbed_llava: The face of the 2010 Winter Olympics mascot, Miga, has red and white colors.\n",
      "3, A_perturbed: {'3': 'red'}, A_perturbed_gpt: {'3': 'red'}\n",
      "Q: \"During the fireworks presentation of the London Olympics what colors lit up the exterior walls of the arena?\"\n",
      "A: ['\"At the fireworks presentation of the London Olympics, the exterior walls of the arena lit up green and blue.\"']\n",
      "A_llava: The exterior walls of the arena lit up with yellow, green, and red colors during the fireworks presentation of the London Olympics.\n",
      "A_llava_blank: The exterior walls of the arena lit up with red, white, and blue colors during the fireworks presentation of the London Olympics.\n",
      "A_perturbed_llava: The exterior walls of the arena lit up with red, white, and blue colors during the fireworks presentation of the London Olympics.\n",
      "2, A_perturbed: {'2': 'violet'}, A_perturbed_gpt: {'2': 'violet'}\n",
      "Q: \"What color is the metal work at the stairs of the Grand Palais?\"\n",
      "A: ['\"The metal work at the stairs of the Grand Palais is yellow, green and black.\"']\n",
      "A_llava: The metal work at the stairs of the Grand Palais is yellow.\n",
      "A_llava_blank: The metal work at the stairs of the Grand Palais is black.\n",
      "A_perturbed_llava: The metal work at the stairs of the Grand Palais is yellow.\n",
      "0, A_perturbed: {'0': 'orange', '1': 'gold', '3': 'gold'}, A_perturbed_gpt: {'0': 'orange', '1': 'gold', '3': 'gold'}\n",
      "Q: \"What colors can be found on the windows of the Riverside Plaza in Minneapolis, Minnesota?\"\n",
      "A: ['\"Red, blue, white, yellow, and grey are colors that can be found on the windows of the Riverside Plaza in Minneapolis, Minnesota.\"']\n",
      "A_llava: The windows of the Riverside Plaza in Minneapolis, Minnesota, feature a variety of colors, including red, blue, green, yellow, and white.\n",
      "A_llava_blank: The windows of the Riverside Plaza in Minneapolis, Minnesota, have white and black colors.\n",
      "A_perturbed_llava: The windows of the Riverside Plaza in Minneapolis, Minnesota, have purple and white colors.\n",
      "2, A_perturbed: {'2': 'purple'}, A_perturbed_gpt: {'2': 'purple'}\n",
      "Q: \"What color is the Golden Gate Bridge's cables ?\"\n",
      "A: ['\"The Golden Gate Bridge\\'s cables are red.\"']\n",
      "A_llava: The Golden Gate Bridge's cables are red.\n",
      "A_llava_blank: The Golden Gate Bridge's cables are red.\n",
      "A_perturbed_llava: The Golden Gate Bridge's cables are red.\n",
      "1, A_perturbed: {'1': 'pink'}, A_perturbed_gpt: {'1': 'pink'}\n",
      "Q: \"What colors are the stadium seats at Camp Nou in Barcelona?\"\n",
      "A: ['\"The stadium seats at Camp Nou in Barcelona are blue and red.\"']\n",
      "A_llava: The stadium seats at Camp Nou in Barcelona are blue and yellow.\n",
      "A_llava_blank: The stadium seats at Camp Nou in Barcelona are blue and white.\n",
      "A_perturbed_llava: The stadium seats at Camp Nou in Barcelona are blue and white.\n",
      "0, A_perturbed: {'0': 'blue'}, A_perturbed_gpt: {'0': 'blue'}\n",
      "Q: \"What color is the word across the top floor of the Enbridge Centre?\"\n",
      "A: ['\"The word across the top floor of the Enbridge Centre is white.\"']\n",
      "A_llava: Yellow\n",
      "A_llava_blank: White\n",
      "A_perturbed_llava: White\n",
      "0, A_perturbed: {'0': 'teal', '1': 'purple', '2': 'gold'}, A_perturbed_gpt: {'0': 'teal', '1': 'purple', '2': 'gold'}\n",
      "Q: \"What colors was the Egypt Pavilion at the World Expo 2010?\"\n",
      "A: ['\"The Egypt Pavilion at the World Expo 2010 was white and black.\"']\n",
      "A_llava: The Egypt Pavilion at the World Expo 2010 was painted in black and white.\n",
      "A_llava_blank: The Egypt Pavilion at the World Expo 2010 was white and black.\n",
      "A_perturbed_llava: The Egypt Pavilion at the World Expo 2010 was pink and white.\n",
      "3, A_perturbed: {'0': 'orange', '3': 'pink'}, A_perturbed_gpt: {'0': 'orange', '3': 'pink'}\n",
      "Q: \"What colors are the tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo?\"\n",
      "A: ['\"There is red and white.\"']\n",
      "A_llava: The tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo, is red and white.\n",
      "A_llava_blank: The tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo is white.\n",
      "A_perturbed_llava: The tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo, is brown and white.\n",
      "1, A_perturbed: {'0': 'white', '1': 'golden'}, A_perturbed_gpt: {'0': 'white', '1': 'golden'}\n",
      "Q: \"What colors were the uniforms for the Torch runners in 2008?\"\n",
      "A: ['\"They were red and white.\"']\n",
      "A_llava: The uniforms for the Torch runners in 2008 were red, white, and blue.\n",
      "A_llava_blank: The uniforms for the Torch runners in 2008 were red, white, and blue.\n",
      "A_perturbed_llava: The uniforms for the Torch runners in 2008 were red and white.\n",
      "1, A_perturbed: {'1': 'brown'}, A_perturbed_gpt: {'1': 'brown'}\n",
      "Q: \"What is the color of the shoes worn by the man in the sculpture \"Blue Coat Scholar\"?\"\n",
      "A: ['\"Blue shoes are worn by the man in the sculpture \"Blue Coat Scholar\".\"']\n",
      "A_llava: The shoes worn by the man in the sculpture \"Blue Coat Scholar\" are yellow.\n",
      "A_llava_blank: The man in the sculpture \"Blue Coat Scholar\" is wearing black shoes.\n",
      "A_perturbed_llava: The shoes worn by the man in the sculpture \"Blue Coat Scholar\" are red.\n",
      "0, A_perturbed: {'0': 'red'}, A_perturbed_gpt: {'0': 'red'}\n",
      "Q: \"What is the color of the hat worn by the woman in Edvard Munch's \"Woman in Blue against Blue Water\"?\"\n",
      "A: ['\"The woman in Edvard Munch\\'s \"Woman in Blue against Blue Water\" is wearing a red hat.\"']\n",
      "A_llava: The woman in Edvard Munch's \"Woman in Blue against Blue Water\" is wearing a red hat.\n",
      "A_llava_blank: The color of the hat worn by the woman in Edvard Munch's \"Woman in Blue against Blue Water\" is blue.\n",
      "A_perturbed_llava: The woman in Edvard Munch's \"Woman in Blue against Blue Water\" is wearing a blue hat.\n",
      "3, A_perturbed: {'3': 'gray'}, A_perturbed_gpt: {'3': 'gray'}\n",
      "Q: \"What colors are the giant tennis racket at Disney's All Star Sports resort?\"\n",
      "A: ['\"The giant tennis racket at Disney\\'s All Star Sports resort is pink and blue.\"']\n",
      "A_llava: The giant tennis racket at Disney's All Star Sports resort is pink and purple.\n",
      "A_llava_blank: The giant tennis racket at Disney's All Star Sports resort is red and blue.\n",
      "A_perturbed_llava: The giant tennis racket at Disney's All Star Sports resort is blue and yellow.\n",
      "0, A_perturbed: {'0': 'orange'}, A_perturbed_gpt: {'0': 'orange'}\n",
      "Q: \"What color are the tips of the Purple coneflower?\"\n",
      "A: ['\"The tips of the purple coneflower are white.\"']\n",
      "A_llava: The tips of the Purple coneflower are yellow.\n",
      "A_llava_blank: The tips of the Purple coneflower are purple.\n",
      "A_perturbed_llava: The tips of the Purple coneflower are pink.\n",
      "2, A_perturbed: {'2': 'pink'}, A_perturbed_gpt: {'2': 'pink'}\n",
      "Q: \"What color warmup jacket did the gold medal winner of the 61 kg boys karate event wear at the 2018 summer youth Olympics?\"\n",
      "A: ['\"The gold medal winner of the 61 kg boys karate event at the 2018 Summer Youth Olympics wore a green warmup jacket.\"']\n",
      "A_llava: The gold medal winner of the 61 kg boys karate event at the 2018 summer youth Olympics wore a red warmup jacket.\n",
      "A_llava_blank: The gold medal winner of the 61 kg boys karate event at the 2018 summer youth Olympics wore a white warmup jacket.\n",
      "A_perturbed_llava: The gold medal winner of the 61 kg boys karate event at the 2018 summer youth Olympics wore an orange warmup jacket.\n",
      "3, A_perturbed: {'1': 'red', '2': 'golden', '3': 'orange'}, A_perturbed_gpt: {'1': 'red', '2': 'golden', '3': 'orange'}\n",
      "Q: \"What colors are the flowers at the center of the Lantana plant that was photographed in August 2007?\"\n",
      "A: ['\"The flowers at the center of the Lantana plant are yellow.\"']\n",
      "A_llava: The flowers at the center of the Lantana plant are pink and yellow.\n",
      "A_llava_blank: The flowers at the center of the Lantana plant are yellow.\n",
      "A_perturbed_llava: The flowers at the center of the Lantana plant are yellow.\n",
      "3, A_perturbed: {'3': 'yellow'}, A_perturbed_gpt: {'3': 'yellow'}\n",
      "Q: \"What colors are on Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon?\"\n",
      "A: ['\"His uniform has black, white, red, and green.\"']\n",
      "A_llava: Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon is red and white.\n",
      "A_llava_blank: White\n",
      "A_perturbed_llava: Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon is grey and black.\n",
      "3, A_perturbed: {'3': 'gray'}, A_perturbed_gpt: {'3': 'gray'}\n",
      "Q: \"What color are the tips of the wings of a Tailed Net-winged Beetle?\"\n",
      "A: ['\"The tips of the wings of a Tailed Net-winged Beetle are colored black.\"']\n",
      "A_llava: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "A_llava_blank: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "A_perturbed_llava: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "3, A_perturbed: {'3': 'red'}, A_perturbed_gpt: {'3': 'red'}\n",
      "Q: \"What color scheme are the trams that move people around Zurich painted?\"\n",
      "A: ['\"Blue and white are the color scheme of the trams that move people around Zurich\"']\n",
      "A_llava: The trams that move people around Zurich are painted in a blue and white color scheme.\n",
      "A_llava_blank: The trams that move people around Zurich are painted in a white and black color scheme.\n",
      "A_perturbed_llava: The trams that move people around Zurich are painted in a white and black color scheme.\n",
      "2, A_perturbed: {'2': 'black'}, A_perturbed_gpt: {'2': 'black'}\n",
      "Q: \"What color is the triangle with the largest area in Vassily Kandinsky's Blue Painting?\"\n",
      "A: ['\"The triangle with the largest area in Vassily Kandinsky\\'s Blue Painting is grey.\"']\n",
      "A_llava: The triangle with the largest area in Vassily Kandinsky's Blue Painting is red.\n",
      "A_llava_blank: The triangle with the largest area in Vassily Kandinsky's Blue Painting is blue.\n",
      "A_perturbed_llava: The triangle with the largest area in Vassily Kandinsky's Blue Painting is red.\n",
      "2, A_perturbed: {'2': 'red'}, A_perturbed_gpt: {'2': 'red'}\n",
      "Q: \"What colors are the squares in Kazimir Malevich's Suprematist painting?\"\n",
      "A: ['\"The colors of the squares in Kazimir Malevich\\'s Suprematist painting are red and black.\"']\n",
      "A_llava: The squares in Kazimir Malevich's Suprematist painting are black, yellow, and red.\n",
      "A_llava_blank: The squares in Kazimir Malevich's Suprematist painting are black and white.\n",
      "A_perturbed_llava: The squares in Kazimir Malevich's Suprematist painting are pink and black.\n",
      "2, A_perturbed: {'2': 'pink'}, A_perturbed_gpt: {'2': 'pink'}\n",
      "Q: \"In the painting Korovin On Balcony, what color are the women's dresses?\"\n",
      "A: ['\"The women\\'s dresses in the painting Korovin On Balcony are black and white.\"']\n",
      "A_llava: The women's dresses are white.\n",
      "A_llava_blank: The women's dresses in the painting are white.\n",
      "A_perturbed_llava: The women in the painting are wearing white dresses.\n",
      "3, A_perturbed: {'2': 'purple', '3': 'black'}, A_perturbed_gpt: {'2': 'purple', '3': 'black'}\n",
      "Q: \"What colors are the seats at Montreal Alouettes games?\"\n",
      "A: ['\"The seats at Montreal Alouettes games are blue and yellow.\"']\n",
      "A_llava: The seats at Montreal Alouettes games are blue and yellow.\n",
      "A_llava_blank: The seats at Montreal Alouettes games are blue and white.\n",
      "A_perturbed_llava: The seats at Montreal Alouettes games are red and blue.\n",
      "2, A_perturbed: {'2': 'red'}, A_perturbed_gpt: {'2': 'red'}\n",
      "Q: \"What color manes and tails are on the horses in Two Horsemen in a Landscape Painting?\"\n",
      "A: ['\"In \"Two Horsemen in a Landscape Painting\", the horses have red and black manes and tails.\"']\n",
      "A_llava: The horses in Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "A_llava_blank: The horses in Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "A_perturbed_llava: The horses in Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "2, A_perturbed: {'2': 'purple'}, A_perturbed_gpt: {'2': 'purple'}\n",
      "Q: \"What color are the stripes that adorn the antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas?\"\n",
      "A: ['\"The stripes that adorn the antennas of the Reakirt\\'s Blue, Echinargus isola butterfly are white.\"']\n",
      "A_llava: The stripes on the antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas are purple.\n",
      "A_llava_blank: The stripes on the antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas are black.\n",
      "A_perturbed_llava: The antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas, have red stripes.\n",
      "0, A_perturbed: {'0': 'red'}, A_perturbed_gpt: {'0': 'red'}\n",
      "Q: \"What color feathers surround more of the eye of the Bird of paradise: black or yellow?\"\n",
      "A: ['\"There are more yellow feathers than black feathers surrounding the eyes of the Bird of Paradise.\"']\n",
      "A_llava: Black\n",
      "A_llava_blank: Black\n",
      "A_perturbed_llava: Black\n",
      "3, A_perturbed: {'3': 'yellow'}, A_perturbed_gpt: {'3': 'yellow'}\n",
      "Q: \"What colors were shared between both the hat and the shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018?\"\n",
      "A: ['\"Red, white, and black were shared between both the hat and the shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018.\"']\n",
      "A_llava: The hat worn by Kim A-lang was red, and the shirt she was wearing was white.\n",
      "A_llava_blank: The hat and shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018, both had white colors.\n",
      "A_perturbed_llava: The hat worn by Kim A-lang was white, and the shirt she was wearing was also white.\n",
      "3, A_perturbed: {'0': 'blue', '3': 'white'}, A_perturbed_gpt: {'0': 'blue', '3': 'white'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for key in parametric_ans_keys:\n",
    "    print(f\"Q: {eval_data[key]['Q']}\")\n",
    "    print(f\"A: {eval_data[key]['A']}\")\n",
    "    print(f\"A_llava: {eval_data[key]['A_llava']}\")\n",
    "    print(f\"A_llava_blank: {eval_data[key]['A_llava_blank']}\")\n",
    "    idx = perturbation_ignored[key]\n",
    "    print(f\"A_perturbed_llava: {eval_data[key]['A_perturbed_llava'][idx]}\")\n",
    "    \n",
    "    print(f\"{idx}, A_perturbed: {eval_data[key]['A_perturbed']}, A_perturbed_gpt: {eval_data[key]['A_perturbed_gpt']}\")\n",
    "    # , A_llava_blank: {eval_data[key]['A_llava_blank']}, A_perturbed_llava: {eval_data[key]['A_perturbed_llava']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

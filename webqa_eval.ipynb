{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val generations:  901\n",
      "Val match generations:  86\n",
      "Train generations:  7751\n",
      "Train match generations:  890\n",
      "{'color': 572, 'shape': 87}\n",
      "572\n"
     ]
    }
   ],
   "source": [
    "from eval_1022 import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# train_val_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3\", \"r\"))\n",
    "\n",
    "train_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3\", \"r\"))\n",
    "val_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3.val.gpt.json\", \"r\"))\n",
    "\n",
    "final_dataset = {}\n",
    "\n",
    "val_generations = 0\n",
    "train_generations = 0\n",
    "\n",
    "val_match_generations = 0\n",
    "train_match_generations = 0\n",
    "\n",
    "exp_name = \"llava\"\n",
    "\n",
    "# merge train and val generated images\n",
    "for k, v in val_dataset.items():\n",
    "    if val_dataset[k]['split'] == 'val':\n",
    "        if 'A_perturbed' not in v:\n",
    "            continue\n",
    "        val_generations += len(v['A_perturbed'])\n",
    "        unmatched_idxs = []\n",
    "        for label_idx in v['A_perturbed']:\n",
    "            gen_label = v['A_perturbed'][label_idx]\n",
    "            if label_idx not in v['A_perturbed_gpt']:\n",
    "                unmatched_idxs.append(label_idx)\n",
    "            else:\n",
    "                gpt_label = v['A_perturbed_gpt'][label_idx]\n",
    "                if gen_label[:3] == gpt_label[:3]:\n",
    "                    val_match_generations += 1\n",
    "                    final_dataset[k] = v\n",
    "                else:\n",
    "                    unmatched_idxs.append(label_idx)\n",
    "        # remove unmatched idxs\n",
    "        for idx in unmatched_idxs:\n",
    "            del v['A_perturbed'][idx]\n",
    "            if idx in v['A_perturbed_gpt']:\n",
    "                del v['A_perturbed_gpt'][idx]\n",
    "\n",
    "for k, v in train_dataset.items():\n",
    "    if train_dataset[k]['split'] == 'train':\n",
    "        if 'A_perturbed' not in v:\n",
    "            continue\n",
    "        train_generations += len(v['A_perturbed'])\n",
    "        unmatched_idxs = []\n",
    "        # fix A_perturbed_gpt idxs off by one\n",
    "        v['A_perturbed_gpt'] = { str(int(k) - 1): v for k, v in v['A_perturbed_gpt'].items() }\n",
    "        for label_idx in v['A_perturbed']:\n",
    "            gen_label = v['A_perturbed'][label_idx]\n",
    "            if label_idx not in v['A_perturbed_gpt']:\n",
    "                unmatched_idxs.append(label_idx)\n",
    "            else: \n",
    "                gpt_label = v['A_perturbed_gpt'][label_idx]\n",
    "                if gen_label[:3] == gpt_label[:3]:\n",
    "                    train_match_generations += 1\n",
    "                    final_dataset[k] = v\n",
    "                else:\n",
    "                    unmatched_idxs.append(label_idx)\n",
    "        # remove unmatched idxs\n",
    "        for idx in unmatched_idxs:\n",
    "            del v['A_perturbed'][idx]\n",
    "            if idx in v['A_perturbed_gpt']:\n",
    "                del v['A_perturbed_gpt'][idx]\n",
    "\n",
    "print(\"Val generations: \", val_generations)\n",
    "print(\"Val match generations: \", val_match_generations)\n",
    "print(\"Train generations: \", train_generations)\n",
    "print(\"Train match generations: \", train_match_generations)\n",
    "\n",
    "# json.dump(final_dataset, open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_lama.json\", \"w\"))\n",
    "\n",
    "# count keys in each qcate\n",
    "qcate_count = {}\n",
    "for k, v in final_dataset.items():\n",
    "    qcate = v['Qcate'].lower()\n",
    "    if qcate not in qcate_count:\n",
    "        qcate_count[qcate] = 0\n",
    "    qcate_count[qcate] += 1\n",
    "    \n",
    "print(qcate_count)\n",
    "\n",
    "eval_data = {k:v for k, v in final_dataset.items() if v['Qcate'].lower() in ['color']}#, 'shape', 'yesno', 'number']}\n",
    "\n",
    "# json.dump(eval_data, open(f\"WebQA_train_val_color_gpt_matched_{exp_name}.json\", \"w\"))\n",
    "\n",
    "print(len(eval_data))\n",
    "sum([len(v['A_perturbed']) for k, v in eval_data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model, eval\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "disable_torch_init()\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name\n",
    ")\n",
    "\n",
    "def llava_eval_on_webqa_sample(question, image_files, webqa_image_loader = True):\n",
    "    args = type('Args', (), {\n",
    "        \"model_path\": model_path,\n",
    "        \"model_base\": None,\n",
    "        \"model_name\": model_name,\n",
    "        \"query\": question,\n",
    "        \"conv_mode\": None,\n",
    "        \"image_file\": image_files,\n",
    "        \"sep\": \",\",\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": None,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"webqa\": webqa_image_loader,\n",
    "    })()\n",
    "\n",
    "    return eval(tokenizer, model, image_processor,  args)\n",
    "\n",
    "def webqa_accuracy(answer, label, Qcate):\n",
    "    if Qcate == 'color':\n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", color_set)\n",
    "    elif Qcate == 'shape': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", shape_set)\n",
    "    elif Qcate == 'yesno': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", yesno_set)\n",
    "    elif Qcate == 'number': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", {\"NUMBER\"})\n",
    "    else:\n",
    "        return None\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)\n",
    "\n",
    "def accuracy_agg_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in single_image_keys])\n",
    "    two_image_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in two_image_keys])\n",
    "    avr_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items()])\n",
    "    return (single_acc, two_image_acc, avr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAVA baseline on original images that have perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 572/572 [07:49<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7189327485380118, 0.6530172413793104, 0.705565268065268)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "llava_results_baseline = {}\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    question = eval_data[k]['Q']\n",
    "    image_files = ','.join([str(img_data['image_id']) for img_data in eval_data[k]['img_posFacts']])\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(question, image_files, True)\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_baseline[k] = webqa_accuracy(answer, label, Qcate)\n",
    "\n",
    "print(accuracy_agg_results(llava_results_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 572/572 [07:38<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2978801169590643, 0.3712643678160919, 0.3127622377622377)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_results_blank = {}\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    question = eval_data[k]['Q']\n",
    "    image_files ='playground/counterfactual_exp/BLANK.jpg'\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(question, image_files, False)\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava_blank'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_blank[k] = webqa_accuracy(answer, label, Qcate)\n",
    "print(accuracy_agg_results(llava_results_blank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 572/572 [11:33<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label = {}\n",
    "llava_results_perturbed_generated_label = {}\n",
    "\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    llava_results_perturbed_original_label[k] = {}\n",
    "    llava_results_perturbed_generated_label[k] = {}\n",
    "    eval_data[k]['A_perturbed_llava'] = {}\n",
    "    question = eval_data[k]['Q']\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "        # TODO: add image 2 (image loader can't handle this now)\n",
    "        # image_files = ','.join([str(img_data['image_id']) for img_data in eval_data[k]['img_posFacts']])\n",
    "        image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "        try:\n",
    "            answer = llava_eval_on_webqa_sample(question, image_files, False)\n",
    "        except:\n",
    "            answer = 'ERROR: llava failed'\n",
    "\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llava'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llava_results_perturbed_original_label[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llava_results_perturbed_generated_label[k][idx] = webqa_accuracy(answer, [label], Qcate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13203463203463203, 0.15963855421686746, 0.13736903376018625)\n",
      "(0.7085137085137085, 0.6882530120481928, 0.7045983701979045)\n"
     ]
    }
   ],
   "source": [
    "def accuracy_agg_generated_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in single_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    two_image_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in two_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    avr_acc = np.mean([PR_avg for key, dict in qa_results.items() for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    \n",
    "    return (single_acc, two_image_acc, avr_acc)\n",
    "\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_original_label))\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_generated_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>single_image</th>\n",
       "      <th>two_image</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.718933</td>\n",
       "      <td>0.653017</td>\n",
       "      <td>0.705565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blank</td>\n",
       "      <td>0.297880</td>\n",
       "      <td>0.371264</td>\n",
       "      <td>0.312762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perturbed_original_label</td>\n",
       "      <td>0.132035</td>\n",
       "      <td>0.159639</td>\n",
       "      <td>0.137369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perturbed_generated_label</td>\n",
       "      <td>0.708514</td>\n",
       "      <td>0.688253</td>\n",
       "      <td>0.704598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             experiment name  single_image  two_image   average\n",
       "0                   baseline      0.718933   0.653017  0.705565\n",
       "1                      blank      0.297880   0.371264  0.312762\n",
       "2   perturbed_original_label      0.132035   0.159639  0.137369\n",
       "3  perturbed_generated_label      0.708514   0.688253  0.704598"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_accs = accuracy_agg_results(llava_results_baseline)\n",
    "blank_accs = accuracy_agg_results(llava_results_blank)\n",
    "perturbed_original_label_accs = accuracy_agg_generated_results(llava_results_perturbed_original_label)\n",
    "perturbed_generated_label_acc = accuracy_agg_generated_results(llava_results_perturbed_generated_label)\n",
    "\n",
    "columns = ['experiment name', 'single_image', 'two_image', 'average']\n",
    "accuracy_agg_df = pd.DataFrame(columns=columns)\n",
    "accuracy_agg_df['experiment name'] = ['baseline', 'blank', 'perturbed_original_label', 'perturbed_generated_label']\n",
    "accuracy_agg_df['single_image'] = [baseline_accs[0], blank_accs[0], perturbed_original_label_accs[0], perturbed_generated_label_acc[0]]\n",
    "accuracy_agg_df['two_image'] = [baseline_accs[1], blank_accs[1], perturbed_original_label_accs[1], perturbed_generated_label_acc[1]]\n",
    "accuracy_agg_df['average'] = [baseline_accs[2], blank_accs[2], perturbed_original_label_accs[2], perturbed_generated_label_acc[2]]\n",
    "accuracy_agg_df.to_csv(\"results/{exp_name}.csv\", index=False)\n",
    "accuracy_agg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image and perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for questions that llava gets right regardless of having a blank image, check accuracy on counterfactual examples\n",
    "perturbation_ignored = {key:idx for key, dict in llava_results_perturbed_original_label.items() for idx, (_,_,_,_,acc) in dict.items() if acc > 0}\n",
    "# and accuray on llava_results_perturbed_generated_label is 0\n",
    "perturbation_ignored = {key:idx for key, idx in perturbation_ignored.items() if llava_results_perturbed_generated_label[key][idx][4] == 0}\n",
    "blank_ignored = set([k for k, (_,_,_,_,acc) in llava_results_blank.items() if acc > 0])\n",
    "parametric_ans_keys = set(perturbation_ignored.keys()).intersection(blank_ignored)\n",
    "len(parametric_ans_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: d5ca49d80dba11ecb1e81171463288e9, Q: \"What color are the walls at The Blue Hour Contemporary Art Gallery in Vancouver?\"\n",
      "A: ['\"The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\"']\n",
      "A_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_llava_blank: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_perturbed_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are black.\n",
      "2, A_perturbed: gray\n",
      "Key: d5cf04aa0dba11ecb1e81171463288e9, Q: \"What color is the word across the top floor of the Enbridge Centre?\"\n",
      "A: ['\"The word across the top floor of the Enbridge Centre is white.\"']\n",
      "A_llava: The word across the top floor of the Enbridge Centre is yellow.\n",
      "A_llava_blank: The word \"Enbridge\" is white.\n",
      "A_perturbed_llava: The word across the top floor of the Enbridge Centre is white.\n",
      "0, A_perturbed: teal\n",
      "Key: d5c8efca0dba11ecb1e81171463288e9, Q: \"What color feathers surround more of the eye of the Bird of paradise: black or yellow?\"\n",
      "A: ['\"There are more yellow feathers than black feathers surrounding the eyes of the Bird of Paradise.\"']\n",
      "A_llava: The Bird of paradise has black feathers surrounding more of its eye.\n",
      "A_llava_blank: The Bird of Paradise has black feathers surrounding more of its eye.\n",
      "A_perturbed_llava: The Bird of paradise has black feathers surrounding more of its eye.\n",
      "3, A_perturbed: yellow\n",
      "Key: d5c4bc980dba11ecb1e81171463288e9, Q: \"What color is the Golden Gate Bridge's cables ?\"\n",
      "A: ['\"The Golden Gate Bridge\\'s cables are red.\"']\n",
      "A_llava: The Golden Gate Bridge's cables are red.\n",
      "A_llava_blank: The Golden Gate Bridge's cables are painted in a shade of red, which is known as \"International Orange.\"\n",
      "A_perturbed_llava: The Golden Gate Bridge's cables are red.\n",
      "1, A_perturbed: pink\n",
      "Key: d5c8718a0dba11ecb1e81171463288e9, Q: \"What colors were the uniforms for the Torch runners in 2008?\"\n",
      "A: ['\"They were red and white.\"']\n",
      "A_llava: The uniforms for the Torch runners in 2008 were red, white, and blue.\n",
      "A_llava_blank: The uniforms for the Torch runners in 2008 were white and black.\n",
      "A_perturbed_llava: The Torch runners in 2008 wore red and white uniforms.\n",
      "1, A_perturbed: brown\n",
      "Key: d5c8e7780dba11ecb1e81171463288e9, Q: \"What colors are the squares in Kazimir Malevich's Suprematist painting?\"\n",
      "A: ['\"The colors of the squares in Kazimir Malevich\\'s Suprematist painting are red and black.\"']\n",
      "A_llava: In Kazimir Malevich's Suprematist painting, the squares are black and yellow.\n",
      "A_llava_blank: In Kazimir Malevich's Suprematist painting, the squares are black and white.\n",
      "A_perturbed_llava: The squares in Kazimir Malevich's Suprematist painting are black and white.\n",
      "2, A_perturbed: pink\n",
      "Key: d5c3b50a0dba11ecb1e81171463288e9, Q: \"What colors are the giant tennis racket at Disney's All Star Sports resort?\"\n",
      "A: ['\"The giant tennis racket at Disney\\'s All Star Sports resort is pink and blue.\"']\n",
      "A_llava: The giant tennis racket at Disney's All Star Sports resort is pink and purple.\n",
      "A_llava_blank: The giant tennis racket at Disney's All Star Sports resort is blue and white.\n",
      "A_perturbed_llava: The giant tennis racket at Disney's All Star Sports resort is blue and yellow.\n",
      "0, A_perturbed: orange\n",
      "Key: d5c395ca0dba11ecb1e81171463288e9, Q: \"What is the color of the taxis in the Times Square?\"\n",
      "A: ['\"The taxis in the Times Square are yellow.\"']\n",
      "A_llava: The color of the taxis in the Times Square is yellow.\n",
      "A_llava_blank: The color of the taxis in the Times Square is yellow.\n",
      "A_perturbed_llava: The color of the taxis in the Times Square is yellow.\n",
      "2, A_perturbed: orange\n",
      "Key: d5c4a4100dba11ecb1e81171463288e9, Q: \"What color is the sign on the Mayo Civic Center?\"\n",
      "A: ['\"The sign on the Mayo Civic Center is white.\"']\n",
      "A_llava: The sign on the Mayo Civic Center is red.\n",
      "A_llava_blank: The sign on the Mayo Civic Center is white.\n",
      "A_perturbed_llava: The sign on the Mayo Civic Center is white.\n",
      "2, A_perturbed: gray\n",
      "Key: d5ce204e0dba11ecb1e81171463288e9, Q: \"During the fireworks presentation of the London Olympics what colors lit up the exterior walls of the arena?\"\n",
      "A: ['\"At the fireworks presentation of the London Olympics, the exterior walls of the arena lit up green and blue.\"']\n",
      "A_llava: During the fireworks presentation of the London Olympics, the exterior walls of the arena were lit up with yellow, green, and red colors.\n",
      "A_llava_blank: The exterior walls of the arena during the fireworks presentation of the London Olympics were lit up with red, white, and blue colors. These colors are the national colors of the United Kingdom, and they were used to create a patriotic and festive atmosphere during the event. The combination of red, white, and blue is also known as the Union Jack, which is the national flag of the United Kingdom.\n",
      "A_perturbed_llava: During the fireworks presentation of the London Olympics, the exterior walls of the arena lit up with a variety of colors, including red, white, and blue. These colors are the national colors of the United Kingdom, and they were used to create a spectacular display for the audience. The fireworks show was a part of the opening ceremony of the 2012 London Olympics, and it was designed to celebrate the spirit of the games and the host country.\n",
      "2, A_perturbed: violet\n",
      "Key: d5c9a2f80dba11ecb1e81171463288e9, Q: \"What color is the backdrop behind the golden gun displayed at the International Spy Museum?\"\n",
      "A: ['\"The backdrop behind the golden gun displayed at the International Spy Museum is red.\"']\n",
      "A_llava: The backdrop behind the golden gun displayed at the International Spy Museum is red.\n",
      "A_llava_blank: The backdrop behind the golden gun displayed at the International Spy Museum is white.\n",
      "A_perturbed_llava: The backdrop behind the golden gun displayed at the International Spy Museum is green.\n",
      "1, A_perturbed: teal\n",
      "Key: d5c96f220dba11ecb1e81171463288e9, Q: \"What color is the sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK?\"\n",
      "A: ['\"The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK is white.\"']\n",
      "A_llava: The sign lettering directly above the entrance of the Royal Court theater is white.\n",
      "A_llava_blank: The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK, is white.\n",
      "A_perturbed_llava: The sign lettering directly above the entrance of the Royal Court theater, Sloane Square Chelsea London UK, is white.\n",
      "2, A_perturbed: gray\n",
      "Key: d5c4fb9a0dba11ecb1e81171463288e9, Q: \"What colors are Charles Talbot's robe in the painting of him by John Vanderbank?\"\n",
      "A: ['\"The colors of Charles Talbot\\'s robe in the painting of him by John Vanderbank are black and gold.\"']\n",
      "A_llava: In the painting of Charles Talbot by John Vanderbank, he is wearing a black robe with gold trim.\n",
      "A_llava_blank: In the painting of Charles Talbot by John Vanderbank, Talbot is depicted wearing a black and white robe.\n",
      "A_perturbed_llava: In the painting of Charles Talbot by John Vanderbank, he is wearing a black and white robe.\n",
      "1, A_perturbed: violet\n",
      "Key: d5c71bbe0dba11ecb1e81171463288e9, Q: \"What color is the metal work at the stairs of the Grand Palais?\"\n",
      "A: ['\"The metal work at the stairs of the Grand Palais is yellow, green and black.\"']\n",
      "A_llava: The metal work at the stairs of the Grand Palais is yellow.\n",
      "A_llava_blank: The metal work at the stairs of the Grand Palais is black.\n",
      "A_perturbed_llava: The metal work at the stairs of the Grand Palais is yellow.\n",
      "0, A_perturbed: orange\n",
      "Key: d5ce3c8c0dba11ecb1e81171463288e9, Q: \"What color are the tips of the wings of a Tailed Net-winged Beetle?\"\n",
      "A: ['\"The tips of the wings of a Tailed Net-winged Beetle are colored black.\"']\n",
      "A_llava: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "A_llava_blank: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "A_perturbed_llava: The tips of the wings of a Tailed Net-winged Beetle are black.\n",
      "3, A_perturbed: red\n",
      "Key: d5cc922e0dba11ecb1e81171463288e9, Q: \"In the painting Korovin On Balcony, what color are the women's dresses?\"\n",
      "A: ['\"The women\\'s dresses in the painting Korovin On Balcony are black and white.\"']\n",
      "A_llava: The women in the painting are wearing white dresses.\n",
      "A_llava_blank: In the painting Korovin On Balcony, the women are wearing white dresses.\n",
      "A_perturbed_llava: In the painting Korovin On Balcony, the women are wearing white dresses.\n",
      "3, A_perturbed: black\n",
      "Key: d5c02f160dba11ecb1e81171463288e9, Q: \"What color on the platform at 8th Avenue Sea Beach station warns passengers where the edge of the platform ends?\"\n",
      "A: ['\"The color yellow is used on the platform at 8th Avenue Sea Beach station to warn passengers where the edge of the platform ends.\"']\n",
      "A_llava: The platform at 8th Avenue Sea Beach station has a yellow line that warns passengers where the edge of the platform ends. This helps passengers to be aware of the platform's boundaries and avoid accidentally falling onto the tracks or getting too close to the edge.\n",
      "A_llava_blank: The platform at 8th Avenue Sea Beach station has a yellow line that warns passengers where the edge of the platform ends. This helps passengers to be aware of the platform's boundaries and avoid accidentally falling off the platform or getting too close to the edge.\n",
      "A_perturbed_llava: The platform at 8th Avenue Sea Beach station has a yellow line that warns passengers where the edge of the platform ends. This helps passengers to be aware of the platform's boundaries and avoid accidentally falling or getting too close to the edge.\n",
      "1, A_perturbed: red\n",
      "Key: d5c9a5fa0dba11ecb1e81171463288e9, Q: \"What color is the border around Malevich's Black Square?\"\n",
      "A: ['\"The border around Malevich\\'s Black Square is white.\"']\n",
      "A_llava: The border around Malevich's Black Square is white.\n",
      "A_llava_blank: The border around Malevich's Black Square is black.\n",
      "A_perturbed_llava: The border around Malevich's Black Square is gold.\n",
      "3, A_perturbed: golden\n",
      "Key: d5cab3aa0dba11ecb1e81171463288e9, Q: \"What color jacket is Mike Tyson wearing at the Heavyweight Exhibit at Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada?\"\n",
      "A: ['\"Mike Tyson wore a black jacket at the Heavyweight Exhibit at the Boxing Hall of Fame in Las Vegas, Nevada.\"']\n",
      "A_llava: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at the Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "A_llava_blank: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at the Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "A_perturbed_llava: Mike Tyson is wearing a black jacket at the Heavyweight Exhibit at the Boxing Hall of Fame, Luxor Hotel, Las Vegas, Nevada.\n",
      "3, A_perturbed: blue\n",
      "Key: d5c7cfb40dba11ecb1e81171463288e9, Q: \"What colors are the tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo?\"\n",
      "A: ['\"There is red and white.\"']\n",
      "A_llava: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is red and white.\n",
      "A_llava_blank: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is white and black.\n",
      "A_perturbed_llava: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is brown and white.\n",
      "1, A_perturbed: golden\n",
      "Key: d5c8d2380dba11ecb1e81171463288e9, Q: \"What color is the mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba?\"\n",
      "A: ['\"The mask on the external wall mural at Rainbow Stage is yellow.\"']\n",
      "A_llava: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba, is gold.\n",
      "A_llava_blank: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba, is white.\n",
      "A_perturbed_llava: The mask on the External wall mural at Rainbow Stage in Kildonan Park, Winnipeg Manitoba, is gold.\n",
      "3, A_perturbed: golden\n",
      "Key: d5c322e80dba11ecb1e81171463288e9, Q: \"What color is the cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul?\"\n",
      "A: ['\"The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\"']\n",
      "A_llava: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "A_llava_blank: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "A_perturbed_llava: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "0, A_perturbed: golden\n",
      "Key: d5c4cfd00dba11ecb1e81171463288e9, Q: \"What color is the text below the white paint splash in the street art war graffiti by Jacek Tylicki?\"\n",
      "A: ['\"The color is the text below the white paint splash in the street art war graffiti by Jacek Tylicki is white.\"']\n",
      "A_llava: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is black.\n",
      "A_llava_blank: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is black.\n",
      "A_perturbed_llava: The text below the white paint splash in the street art war graffiti by Jacek Tylicki is black.\n",
      "1, A_perturbed: yellow\n",
      "Key: d5c849a80dba11ecb1e81171463288e9, Q: \"What is the color of the hat worn by the woman in Edvard Munch's \"Woman in Blue against Blue Water\"?\"\n",
      "A: ['\"The woman in Edvard Munch\\'s \"Woman in Blue against Blue Water\" is wearing a red hat.\"']\n",
      "A_llava: The woman in Edvard Munch's \"Woman in Blue against Blue Water\" is wearing a red hat.\n",
      "A_llava_blank: The woman in Edvard Munch's \"Woman in Blue against Blue Water\" is wearing a black hat.\n",
      "A_perturbed_llava: The woman in Edvard Munch's \"Woman in Blue against Blue Water\" is wearing a blue hat.\n",
      "3, A_perturbed: gray\n",
      "Key: d5c9193c0dba11ecb1e81171463288e9, Q: \"Which color fireworks were present at the closing ceremonies of both the 2012 and 2016 Summer Olympics?\"\n",
      "A: ['\"There were red and yellow fireworks at the closing ceremonies of both the 2012 and 2016 Summer Olympics.\"']\n",
      "A_llava: The fireworks at the closing ceremonies of both the 2012 and 2016 Summer Olympics were predominantly red, white, and blue. These colors are often associated with patriotism and national pride, and they are commonly used in Olympic ceremonies to represent the host country. The fireworks display in the image is a reflection of this tradition, as the red, white, and blue fireworks create a vibrant and memorable spectacle for the audience.\n",
      "A_llava_blank: The closing ceremonies of both the 2012 and 2016 Summer Olympics featured a fireworks display in the colors of the Union Jack, which is the national flag of the United Kingdom. The Union Jack consists of three horizontal crosses representing the colors of the British flag: red (representing England), white (representing Scotland), and blue (representing Ireland, now Northern Ireland). The fireworks display at the closing ceremonies of both the 2012 and 2016 Olympics showcased these colors, symbolizing the United Kingdom's role in hosting the events.\n",
      "A_perturbed_llava: The closing ceremonies of both the 2012 and 2016 Summer Olympics featured fireworks in the colors of red, white, and blue. These colors are often associated with patriotism and national pride, and they are commonly used in Olympic ceremonies to represent the host country. The fireworks display in the image is a part of the closing ceremony for the 2016 Summer Olympics, held in Rio de Janeiro, Brazil.\n",
      "3, A_perturbed: yellow\n",
      "Key: d5c367260dba11ecb1e81171463288e9, Q: \"What color is the world cup trophy?\"\n",
      "A: ['\"The World Cup Trophy is gold with a green base.\"']\n",
      "A_llava: The world cup trophy is gold.\n",
      "A_llava_blank: The world cup trophy is gold.\n",
      "A_perturbed_llava: The world cup trophy is gold.\n",
      "3, A_perturbed: golden\n",
      "Key: d5bc83d40dba11ecb1e81171463288e9, Q: \"What colors are on Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon?\"\n",
      "A: ['\"His uniform has black, white, red, and green.\"']\n",
      "A_llava: Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon features red, white, and black colors.\n",
      "A_llava_blank: Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon features white and black colors.\n",
      "A_perturbed_llava: Marc-Andre Bedard's uniform at the Winter Youth Olympics Biathlon is grey and black.\n",
      "3, A_perturbed: gray\n",
      "Key: d5bf1e6e0dba11ecb1e81171463288e9, Q: \"What color manes and tails are on the horses in Two Horsemen in a Landscape Painting?\"\n",
      "A: ['\"In \"Two Horsemen in a Landscape Painting\", the horses have red and black manes and tails.\"']\n",
      "A_llava: The horses in the Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "A_llava_blank: The horses in Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "A_perturbed_llava: The horses in the Two Horsemen in a Landscape Painting have black manes and tails.\n",
      "2, A_perturbed: purple\n",
      "Key: d5c5e2a80dba11ecb1e81171463288e9, Q: \"What color are some of the walls at the Van Gogh Museum?\"\n",
      "A: ['\"Red, white, yellow, and brown are the colors on some of the walls at the Van Gogh Museum.\"']\n",
      "A_llava: The walls at the Van Gogh Museum are painted yellow.\n",
      "A_llava_blank: The walls at the Van Gogh Museum are painted white.\n",
      "A_perturbed_llava: The walls at the Van Gogh Museum are predominantly white.\n",
      "2, A_perturbed: violet\n",
      "Key: d5c72ed80dba11ecb1e81171463288e9, Q: \"What is the color of the hat worn by Carel Fabricius in his self portrait (done on 1645)?\"\n",
      "A: ['\"The color of the hat worn by Carel Fabricius in his self portrait (done on 1645) is black.\"']\n",
      "A_llava: The color of the hat worn by Carel Fabricius in his self portrait is black.\n",
      "A_llava_blank: Carel Fabricius is known for his self-portrait, which was painted in 1645. In the portrait, he is wearing a black hat.\n",
      "A_perturbed_llava: Carel Fabricius is wearing a black hat in his self-portrait.\n",
      "1, A_perturbed: violet\n",
      "Key: d5c57c8c0dba11ecb1e81171463288e9, Q: \"What color are the leaves that grow on the stems of the Lantana flowers?\"\n",
      "A: ['\"The leaves that grow on the stems of the Lantana flowers are green.\"']\n",
      "A_llava: The leaves that grow on the stems of the Lantana flowers are green.\n",
      "A_llava_blank: The leaves that grow on the stems of the Lantana flowers are green.\n",
      "A_perturbed_llava: The leaves that grow on the stems of the Lantana flowers are green.\n",
      "2, A_perturbed: brown\n",
      "Key: d5c2f5340dba11ecb1e81171463288e9, Q: \"Which colors are found on both a sign at the Hot Doug's restaurant in Chicago and the cups used by Nathan's Famous at Coney Island, New York?\"\n",
      "A: ['\"Red, green, and yellow are on both a sign at the Hot Doug\\'s restaurant in Chicago and the cups used by Nathan\\'s Famous at Coney Island.\"']\n",
      "A_llava: Both the sign at the Hot Doug's restaurant in Chicago and the cups used by Nathan's Famous at Coney Island, New York, feature red and yellow colors.\n",
      "A_llava_blank: Both the sign at the Hot Doug's restaurant in Chicago and the cups used by Nathan's Famous at Coney Island, New York, feature red and white colors.\n",
      "A_perturbed_llava: Both the sign at the Hot Doug's restaurant in Chicago and the cups used by Nathan's Famous at Coney Island, New York, feature red and white colors.\n",
      "2, A_perturbed: pink\n",
      "Key: d5c5bcf60dba11ecb1e81171463288e9, Q: \"What color is the belly of a Green Tree Frog?\"\n",
      "A: ['\"The belly of a Green Tree Frog is white.\"']\n",
      "A_llava: The belly of a Green Tree Frog is white.\n",
      "A_llava_blank: The belly of a Green Tree Frog is white.\n",
      "A_perturbed_llava: The belly of a Green Tree Frog is white.\n",
      "0, A_perturbed: pink\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for key in parametric_ans_keys:\n",
    "    print(f\"Key: {key}, Q: {eval_data[key]['Q']}\")\n",
    "    print(f\"A: {eval_data[key]['A']}\")\n",
    "    print(f\"A_llava: {eval_data[key]['A_llava']}\")\n",
    "    print(f\"A_llava_blank: {eval_data[key]['A_llava_blank']}\")\n",
    "    idx = perturbation_ignored[key]\n",
    "    print(f\"A_perturbed_llava: {eval_data[key]['A_perturbed_llava'][idx]}\")\n",
    "    \n",
    "    print(f\"{idx}, A_perturbed: {eval_data[key]['A_perturbed'][idx]}\")\n",
    "    # , A_llava_blank: {eval_data[key]['A_llava_blank']}, A_perturbed_llava: {eval_data[key]['A_perturbed_llava']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

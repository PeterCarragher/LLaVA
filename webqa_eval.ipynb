{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val generations:  901\n",
      "Val match generations:  86\n",
      "Train generations:  7751\n",
      "Train match generations:  890\n",
      "{'color': 572, 'shape': 87}\n",
      "572\n"
     ]
    }
   ],
   "source": [
    "from eval_1022 import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# train_val_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3\", \"r\"))\n",
    "\n",
    "train_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3\", \"r\"))\n",
    "val_dataset = json.load(open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_obj_v2.json.pert.lama.v3.val.gpt.json\", \"r\"))\n",
    "\n",
    "final_dataset = {}\n",
    "\n",
    "val_generations = 0\n",
    "train_generations = 0\n",
    "\n",
    "val_match_generations = 0\n",
    "train_match_generations = 0\n",
    "\n",
    "exp_name = \"llava\"\n",
    "\n",
    "# merge train and val generated images\n",
    "for k, v in val_dataset.items():\n",
    "    if val_dataset[k]['split'] == 'val':\n",
    "        if 'A_perturbed' not in v:\n",
    "            continue\n",
    "        val_generations += len(v['A_perturbed'])\n",
    "        unmatched_idxs = []\n",
    "        for label_idx in v['A_perturbed']:\n",
    "            gen_label = v['A_perturbed'][label_idx]\n",
    "            if label_idx not in v['A_perturbed_gpt']:\n",
    "                unmatched_idxs.append(label_idx)\n",
    "            else:\n",
    "                gpt_label = v['A_perturbed_gpt'][label_idx]\n",
    "                if gen_label[:3] == gpt_label[:3]:\n",
    "                    val_match_generations += 1\n",
    "                    final_dataset[k] = v\n",
    "                else:\n",
    "                    unmatched_idxs.append(label_idx)\n",
    "        # remove unmatched idxs\n",
    "        for idx in unmatched_idxs:\n",
    "            del v['A_perturbed'][idx]\n",
    "            if idx in v['A_perturbed_gpt']:\n",
    "                del v['A_perturbed_gpt'][idx]\n",
    "\n",
    "for k, v in train_dataset.items():\n",
    "    if train_dataset[k]['split'] == 'train':\n",
    "        if 'A_perturbed' not in v:\n",
    "            continue\n",
    "        train_generations += len(v['A_perturbed'])\n",
    "        unmatched_idxs = []\n",
    "        # fix A_perturbed_gpt idxs off by one\n",
    "        v['A_perturbed_gpt'] = { str(int(k) - 1): v for k, v in v['A_perturbed_gpt'].items() }\n",
    "        for label_idx in v['A_perturbed']:\n",
    "            gen_label = v['A_perturbed'][label_idx]\n",
    "            if label_idx not in v['A_perturbed_gpt']:\n",
    "                unmatched_idxs.append(label_idx)\n",
    "            else: \n",
    "                gpt_label = v['A_perturbed_gpt'][label_idx]\n",
    "                if gen_label[:3] == gpt_label[:3]:\n",
    "                    train_match_generations += 1\n",
    "                    final_dataset[k] = v\n",
    "                else:\n",
    "                    unmatched_idxs.append(label_idx)\n",
    "        # remove unmatched idxs\n",
    "        for idx in unmatched_idxs:\n",
    "            del v['A_perturbed'][idx]\n",
    "            if idx in v['A_perturbed_gpt']:\n",
    "                del v['A_perturbed_gpt'][idx]\n",
    "\n",
    "print(\"Val generations: \", val_generations)\n",
    "print(\"Val match generations: \", val_match_generations)\n",
    "print(\"Train generations: \", train_generations)\n",
    "print(\"Train match generations: \", train_match_generations)\n",
    "\n",
    "# json.dump(final_dataset, open(\"/home/pcarragh/dev/webqa/MultiModalQA/data/WebQA_train_val_lama.json\", \"w\"))\n",
    "\n",
    "# count keys in each qcate\n",
    "qcate_count = {}\n",
    "for k, v in final_dataset.items():\n",
    "    qcate = v['Qcate'].lower()\n",
    "    if qcate not in qcate_count:\n",
    "        qcate_count[qcate] = 0\n",
    "    qcate_count[qcate] += 1\n",
    "    \n",
    "print(qcate_count)\n",
    "\n",
    "eval_data = {k:v for k, v in final_dataset.items() if v['Qcate'].lower() in ['color']}#, 'shape', 'yesno', 'number']}\n",
    "\n",
    "# json.dump(eval_data, open(f\"WebQA_train_val_color_gpt_matched_{exp_name}.json\", \"w\"))\n",
    "\n",
    "print(len(eval_data))\n",
    "sum([len(v['A_perturbed']) for k, v in eval_data.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model, eval\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "disable_torch_init()\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, None, model_name\n",
    ")\n",
    "\n",
    "def llava_eval_on_webqa_sample(question, image_files, webqa_image_loader = True):\n",
    "    args = type('Args', (), {\n",
    "        \"model_path\": model_path,\n",
    "        \"model_base\": None,\n",
    "        \"model_name\": model_name,\n",
    "        \"query\": question,\n",
    "        \"conv_mode\": None,\n",
    "        \"image_file\": image_files,\n",
    "        \"sep\": \",\",\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": None,\n",
    "        \"num_beams\": 1,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"webqa\": webqa_image_loader,\n",
    "    })()\n",
    "\n",
    "    return eval(tokenizer, model, image_processor,  args)\n",
    "\n",
    "def webqa_accuracy(answer, label, Qcate):\n",
    "    if Qcate == 'color':\n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", color_set)\n",
    "    elif Qcate == 'shape': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", shape_set)\n",
    "    elif Qcate == 'yesno': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", yesno_set)\n",
    "    elif Qcate == 'number': \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([answer], label[0], \"\", {\"NUMBER\"})\n",
    "    else:\n",
    "        return None\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)\n",
    "\n",
    "def accuracy_agg_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in single_image_keys])\n",
    "    two_image_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items() if key in two_image_keys])\n",
    "    avr_acc = np.mean([PR_avg for key, (F1_avg, F1_max, EM, RE_avg, PR_avg) in qa_results.items()])\n",
    "    return (single_acc, two_image_acc, avr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAVA baseline on original images that have perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/572 [00:00<?, ?it/s]/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pcarragh/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 31%|███       | 176/572 [02:21<06:39,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "llava_results_baseline = {}\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    question = eval_data[k]['Q']\n",
    "    image_files = ','.join([str(img_data['image_id']) for img_data in eval_data[k]['img_posFacts']])\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(question, image_files, True)\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_baseline[k] = webqa_accuracy(answer, label, Qcate)\n",
    "\n",
    "print(accuracy_agg_results(llava_results_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [03:27<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2940251572327044, 0.3851190476190477, 0.3130597014925373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_results_blank = {}\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    question = eval_data[k]['Q']\n",
    "    image_files ='playground/counterfactual_exp/BLANK.jpg'\n",
    "    try:\n",
    "        answer = llava_eval_on_webqa_sample(question, image_files, False)\n",
    "    except:\n",
    "        answer = 'ERROR: llava failed'\n",
    "    label = eval_data[k]['A']\n",
    "    eval_data[k]['A_llava_blank'] = answer\n",
    "    Qcate = eval_data[k]['Qcate'].lower()\n",
    "    llava_results_blank[k] = webqa_accuracy(answer, label, Qcate)\n",
    "print(accuracy_agg_results(llava_results_blank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [13:20<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "llava_results_perturbed_original_label = {}\n",
    "llava_results_perturbed_generated_label = {}\n",
    "\n",
    "for k in tqdm(list(eval_data.keys())):\n",
    "    llava_results_perturbed_original_label[k] = {}\n",
    "    llava_results_perturbed_generated_label[k] = {}\n",
    "    eval_data[k]['A_perturbed_llava'] = {}\n",
    "    question = eval_data[k]['Q']\n",
    "    for idx, label in eval_data[k]['A_perturbed'].items():\n",
    "        image_id = eval_data[k]['img_posFacts'][0]['image_id']\n",
    "        # TODO: add image 2 (image loader can't handle this now)\n",
    "        # image_files = ','.join([str(img_data['image_id']) for img_data in eval_data[k]['img_posFacts']])\n",
    "        image_files = f\"/home/pcarragh/dev/webqa/segment/Inpaint-Anything/results/webqa/{eval_data[k]['split']}/{str(image_id)}_{k}_{idx}.jpeg\"\n",
    "        try:\n",
    "            answer = llava_eval_on_webqa_sample(question, image_files, False)\n",
    "        except:\n",
    "            answer = 'ERROR: llava failed'\n",
    "\n",
    "        original_label = eval_data[k]['A']\n",
    "        eval_data[k]['A_perturbed_llava'][idx] = answer\n",
    "        Qcate = eval_data[k]['Qcate'].lower()\n",
    "        llava_results_perturbed_original_label[k][idx] = webqa_accuracy(answer, original_label, Qcate)\n",
    "        llava_results_perturbed_generated_label[k][idx] = webqa_accuracy(answer, [label], Qcate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17857142857142858, 0.20317460317460317, 0.18376928236083165)\n",
      "(0.4331420068027211, 0.42460317460317465, 0.43133802816901406)\n"
     ]
    }
   ],
   "source": [
    "def accuracy_agg_generated_results(qa_results):\n",
    "    single_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 1]\n",
    "    two_image_keys = [k for k in qa_results.keys() if len(eval_data[k]['img_posFacts']) == 2]\n",
    "\n",
    "    single_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in single_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    two_image_acc = np.mean([PR_avg for key, dict in qa_results.items() if key in two_image_keys for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    avr_acc = np.mean([PR_avg for key, dict in qa_results.items() for idx, (_,_,_,_,PR_avg) in dict.items()])\n",
    "    \n",
    "    return (single_acc, two_image_acc, avr_acc)\n",
    "\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_original_label))\n",
    "print(accuracy_agg_generated_results(llava_results_perturbed_generated_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment name</th>\n",
       "      <th>single_image</th>\n",
       "      <th>two_image</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.723664</td>\n",
       "      <td>0.610119</td>\n",
       "      <td>0.699938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blank</td>\n",
       "      <td>0.294025</td>\n",
       "      <td>0.385119</td>\n",
       "      <td>0.313060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perturbed_original_label</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.203175</td>\n",
       "      <td>0.183769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perturbed_generated_label</td>\n",
       "      <td>0.433142</td>\n",
       "      <td>0.424603</td>\n",
       "      <td>0.431338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             experiment name  single_image  two_image   average\n",
       "0                   baseline      0.723664   0.610119  0.699938\n",
       "1                      blank      0.294025   0.385119  0.313060\n",
       "2   perturbed_original_label      0.178571   0.203175  0.183769\n",
       "3  perturbed_generated_label      0.433142   0.424603  0.431338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_accs = accuracy_agg_results(llava_results_baseline)\n",
    "blank_accs = accuracy_agg_results(llava_results_blank)\n",
    "perturbed_original_label_accs = accuracy_agg_generated_results(llava_results_perturbed_original_label)\n",
    "perturbed_generated_label_acc = accuracy_agg_generated_results(llava_results_perturbed_generated_label)\n",
    "\n",
    "columns = ['experiment name', 'single_image', 'two_image', 'average']\n",
    "accuracy_agg_df = pd.DataFrame(columns=columns)\n",
    "accuracy_agg_df['experiment name'] = ['baseline', 'blank', 'perturbed_original_label', 'perturbed_generated_label']\n",
    "accuracy_agg_df['single_image'] = [baseline_accs[0], blank_accs[0], perturbed_original_label_accs[0], perturbed_generated_label_acc[0]]\n",
    "accuracy_agg_df['two_image'] = [baseline_accs[1], blank_accs[1], perturbed_original_label_accs[1], perturbed_generated_label_acc[1]]\n",
    "accuracy_agg_df['average'] = [baseline_accs[2], blank_accs[2], perturbed_original_label_accs[2], perturbed_generated_label_acc[2]]\n",
    "accuracy_agg_df.to_csv(\"results/{exp_name}.csv\", index=False)\n",
    "accuracy_agg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct despite blank image and perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for questions that llava gets right regardless of having a blank image, check accuracy on counterfactual examples\n",
    "perturbation_ignored = {key:idx for key, dict in llava_results_perturbed_original_label.items() for idx, (_,_,_,_,acc) in dict.items() if acc > 0}\n",
    "blank_ignored = set([k for k, (_,_,_,_,acc) in llava_results_blank.items() if acc > 0])\n",
    "parametric_ans_keys = set(perturbation_ignored.keys()).intersection(blank_ignored)\n",
    "len(parametric_ans_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: \"What color do the Apatura Iris and Anthocharis Cardamines butterfly species share in terms of wing color?\"\n",
      "A: ['\"They share the color white.\"']\n",
      "A_llava: The Apatura Iris and Anthocharis Cardamines butterfly species share a similar wing color, which is blue.\n",
      "A_llava_blank: The Apatura Iris and Anthocharis Cardamines butterfly species share a similar wing color, which is white.\n",
      "A_perturbed_llava: The Apatura Iris and Anthocharis Cardamines butterfly species share a similar wing color, which is black and white.\n",
      "3, A_perturbed: {'0': 'yellow', '1': 'purple', '2': 'black', '3': 'gray'}, A_perturbed_gpt: {'1': 'yellow'}\n",
      "Q: \"What color are the stripes that adorn the antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas?\"\n",
      "A: ['\"The stripes that adorn the antennas of the Reakirt\\'s Blue, Echinargus isola butterfly are white.\"']\n",
      "A_llava: The antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas, have purple stripes.\n",
      "A_llava_blank: The antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas, have black and white stripes.\n",
      "A_perturbed_llava: The antennas of the Reakirt's Blue, Echinargus isola butterfly found at the National Butterfly Center in Mission, Texas, have black and white stripes.\n",
      "3, A_perturbed: {'0': 'red', '1': 'blue', '2': 'red', '3': 'teal'}, A_perturbed_gpt: {'1': 'red'}\n",
      "Q: \"What color are the walls at The Blue Hour Contemporary Art Gallery in Vancouver?\"\n",
      "A: ['\"The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\"']\n",
      "A_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_llava_blank: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are white.\n",
      "A_perturbed_llava: The walls at The Blue Hour Contemporary Art Gallery in Vancouver are painted red.\n",
      "3, A_perturbed: {'0': 'gray', '1': 'black', '2': 'gray', '3': 'orange'}, A_perturbed_gpt: {'3': 'gray'}\n",
      "Q: \"What color is the sign for the Old Mill Inn Restaurant in Dallas,Texas?\"\n",
      "A: ['\"The sign for the Old Mill Inn Restaurant in Dallas, Texas is red and white.\"']\n",
      "A_llava: The sign for the Old Mill Inn Restaurant in Dallas, Texas is red and white.\n",
      "A_llava_blank: The sign for the Old Mill Inn Restaurant in Dallas, Texas is white.\n",
      "A_perturbed_llava: The sign for the Old Mill Inn Restaurant in Dallas, Texas is red.\n",
      "1, A_perturbed: {'0': 'purple', '1': 'gold', '2': 'violet', '3': 'yellow'}, A_perturbed_gpt: {'1': 'purple', '4': 'yellow'}\n",
      "Q: \"What color shirt can be worn as part of the uniform in a classroom in Niue?\"\n",
      "A: ['\"White or grey shirts are part of the uniform.\"']\n",
      "A_llava: In a classroom in Niue, a white shirt can be worn as part of the uniform.\n",
      "A_llava_blank: In a classroom in Niue, a white shirt can be worn as part of the uniform.\n",
      "A_perturbed_llava: In a classroom in Niue, a white shirt can be worn as part of the uniform.\n",
      "3, A_perturbed: {'0': 'teal', '1': 'red', '2': 'blue', '3': 'purple'}, A_perturbed_gpt: {'0': 'white', '1': 'red', '2': 'white', '3': 'white'}\n",
      "Q: \"What color is the Scholastic building in New York?\"\n",
      "A: ['\"The Scholastic building in New York is white and red.\"']\n",
      "A_llava: The Scholastic building in New York is green.\n",
      "A_llava_blank: The Scholastic building in New York is white.\n",
      "A_perturbed_llava: The Scholastic building in New York is white.\n",
      "0, A_perturbed: {'0': 'white', '1': 'gold', '2': 'teal'}, A_perturbed_gpt: {'1': 'white', '2': 'gold'}\n",
      "Q: \"On the White House Christmas tree, you can find ornaments of what color?\"\n",
      "A: ['\"You can find gold ornaments on the White House Christmas tree.\"']\n",
      "A_llava: The ornaments on the White House Christmas tree are gold.\n",
      "A_llava_blank: The ornaments on the White House Christmas tree are red and white.\n",
      "A_perturbed_llava: On the White House Christmas tree, you can find ornaments of red and white colors.\n",
      "3, A_perturbed: {'0': 'yellow', '1': 'orange', '2': 'red', '3': 'red'}, A_perturbed_gpt: {'1': 'yellow', '2': 'orange', '3': 'red', '4': 'red'}\n",
      "Q: \"What color is the word across the top floor of the Enbridge Centre?\"\n",
      "A: ['\"The word across the top floor of the Enbridge Centre is white.\"']\n",
      "A_llava: The word across the top floor of the Enbridge Centre is yellow.\n",
      "A_llava_blank: The word \"Enbridge\" is white.\n",
      "A_perturbed_llava: The word across the top floor of the Enbridge Centre is white.\n",
      "0, A_perturbed: {'0': 'teal', '1': 'purple', '2': 'gold', '3': 'brown'}, A_perturbed_gpt: {'1': 'teal', '2': 'purple', '3': 'gold'}\n",
      "Q: \"What color shoes did both Hilary Swank and Fergie wear at the opening to the 2013 Life Ball in Vienna, Austria?\"\n",
      "A: ['\"Both Hilary Swank and Fergie wore black shoes at the opening to the 2013 Life Ball in Vienna, Austria.\"']\n",
      "A_llava: Both Hilary Swank and Fergie wore black shoes at the opening to the 2013 Life Ball in Vienna, Austria.\n",
      "A_llava_blank: Both Hilary Swank and Fergie wore black shoes at the opening to the 2013 Life Ball in Vienna, Austria.\n",
      "A_perturbed_llava: Both Hilary Swank and Fergie wore black shoes at the opening to the 2013 Life Ball in Vienna, Austria.\n",
      "3, A_perturbed: {'0': 'golden', '1': 'golden', '2': 'white', '3': 'violet'}, A_perturbed_gpt: {'1': 'golden', '2': 'golden'}\n",
      "Q: \"What colors are found on the Coast Guard boat with number 25500?\"\n",
      "A: ['\"The Coast Guard boat with number 25500 is in red, white, blue, black, and grey.\"']\n",
      "A_llava: The Coast Guard boat with number 25500 has orange and white colors.\n",
      "A_llava_blank: The Coast Guard boat with number 25500 is predominantly white.\n",
      "A_perturbed_llava: The Coast Guard boat with number 25500 is yellow and black.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'purple', '2': 'black', '3': 'yellow'}, A_perturbed_gpt: {'1': 'orange'}\n",
      "Q: \"What is the color of the hair of the ladies in Vincenzo Catena's \"Portrait of a Woman\" and Palma Vecchio's \"Young Woman in blue Gown with Feather\"?\"\n",
      "A: ['\"Both of the ladies in the paintings have blonde hair.\"']\n",
      "A_llava: In Vincenzo Catena's \"Portrait of a Woman,\" the lady has blonde hair. In Palma Vecchio's \"Young Woman in blue Gown with Feather,\" the lady has red hair.\n",
      "A_llava_blank: In Vincenzo Catena's \"Portrait of a Woman,\" the lady has blonde hair. In Palma Vecchio's \"Young Woman in blue Gown with Feather,\" the lady has brown hair.\n",
      "A_perturbed_llava: In Vincenzo Catena's \"Portrait of a Woman,\" the lady has blonde hair. In Palma Vecchio's \"Young Woman in blue Gown with Feather,\" the lady has blonde hair as well.\n",
      "2, A_perturbed: {'0': 'orange', '1': 'red', '2': 'pink', '3': 'red'}, A_perturbed_gpt: {'2': 'red', '4': 'red'}\n",
      "Q: \"What colors was the Egypt Pavilion at the World Expo 2010?\"\n",
      "A: ['\"The Egypt Pavilion at the World Expo 2010 was white and black.\"']\n",
      "A_llava: The Egypt Pavilion at the World Expo 2010 was painted in white, black, and red colors.\n",
      "A_llava_blank: The Egypt Pavilion at the World Expo 2010 was painted in white and blue colors.\n",
      "A_perturbed_llava: The Egypt Pavilion at the World Expo 2010 was painted in pink, white, and green colors.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'violet', '2': 'gray', '3': 'pink'}, A_perturbed_gpt: {'1': 'orange', '4': 'pink'}\n",
      "Q: \"What is the color of the sculpture: Digital Orca in Vancouver?\"\n",
      "A: ['\"The sculpture: Digital Orca in Vancouver is black and white.\"']\n",
      "A_llava: The color of the sculpture is black and white.\n",
      "A_llava_blank: The color of the sculpture is white.\n",
      "A_perturbed_llava: The color of the sculpture is white.\n",
      "0, A_perturbed: {'0': 'gold', '1': 'pink', '2': 'pink', '3': 'golden'}, A_perturbed_gpt: {'2': 'pink', '3': 'pink'}\n",
      "Q: \"What are the colors of the Neumayer Station Antarctica building?\"\n",
      "A: ['\"The Neumayer Station Antarctica building is in red, white, and grey.\"']\n",
      "A_llava: The Neumayer Station Antarctica building is painted in red, white, and blue colors.\n",
      "A_llava_blank: The Neumayer Station Antarctica building is predominantly white, with some black accents.\n",
      "A_perturbed_llava: The Neumayer Station Antarctica building is black and white.\n",
      "3, A_perturbed: {'0': 'yellow', '1': 'gray', '2': 'gold', '3': 'purple'}, A_perturbed_gpt: {'1': 'yellow'}\n",
      "Q: \"What colors are the giant tennis racket at Disney's All Star Sports resort?\"\n",
      "A: ['\"The giant tennis racket at Disney\\'s All Star Sports resort is pink and blue.\"']\n",
      "A_llava: The giant tennis racket at Disney's All Star Sports resort is pink and purple.\n",
      "A_llava_blank: The giant tennis racket at Disney's All Star Sports resort is blue and white.\n",
      "A_perturbed_llava: The giant tennis racket at Disney's All Star Sports resort is blue and black.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'red', '2': 'gold', '3': 'teal'}, A_perturbed_gpt: {'1': 'orange'}\n",
      "Q: \"What color is shared between the flags located on the hats worn by the gold and silver medalists of the 2010 Olympics Women's Snowboard Cross event?\"\n",
      "A: ['\"The flags located on the hats worn by the gold and silver medalists of the 2010 Olympics, Women\\'s Snowboard Cross event, share the color red.\"']\n",
      "A_llava: The gold and silver medalists of the 2010 Olympics Women's Snowboard Cross event are wearing hats with red and white flags on them.\n",
      "A_llava_blank: The color shared between the flags on the hats worn by the gold and silver medalists of the 2010 Olympics Women's Snowboard Cross event is white.\n",
      "A_perturbed_llava: The gold and silver medalists of the 2010 Olympics Women's Snowboard Cross event are wearing red and white hats, which is the same color shared between the flags on their hats.\n",
      "3, A_perturbed: {'0': 'blue', '1': 'red', '2': 'red', '3': 'gold'}, A_perturbed_gpt: {'2': 'red', '3': 'red'}\n",
      "Q: \"What colors are on the children's moccasins featured from 1875 at the Houston Museum of Natural Science?\"\n",
      "A: ['\"The children\\'s moccasins from 1875 featured at the Houston Museum of Natural Science are red, yellow, light blue, and dark blue.\"']\n",
      "A_llava: The children's moccasins featured at the Houston Museum of Natural Science are brown and yellow in color.\n",
      "A_llava_blank: The children's moccasins featured at the Houston Museum of Natural Science are red and white.\n",
      "A_perturbed_llava: The children's moccasins featured at the Houston Museum of Natural Science are yellow and brown in color.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'gray', '2': 'teal', '3': 'yellow'}, A_perturbed_gpt: {'1': 'orange', '4': 'yellow'}\n",
      "Q: \"What colors can the exterior light emit from the Tokyo dome?\"\n",
      "A: ['\"The exterior light can emit the colors blue, green, red, purple, and white.\"']\n",
      "A_llava: The exterior light of the Tokyo Dome can emit a variety of colors, including red, green, blue, and white. In the image, the dome is lit up with these colors, creating a visually striking and vibrant scene. The combination of these colors creates a unique and eye-catching display, which can be seen by people walking around the area outside the dome.\n",
      "A_llava_blank: The exterior light of the Tokyo Dome can emit various colors, including red, white, and blue. These colors are often used to create visually appealing displays and to convey specific messages or emotions. The choice of colors can be influenced by factors such as the event taking place at the dome, the time of day, or even seasonal celebrations.\n",
      "A_perturbed_llava: The exterior light of the Tokyo dome can emit various colors, including red, white, and orange. In the image, the dome is lit up with red and white lights, creating a visually striking scene. The combination of these colors can create a vibrant and dynamic atmosphere, attracting attention and enhancing the overall aesthetic of the building.\n",
      "3, A_perturbed: {'0': 'gold', '1': 'black', '2': 'pink', '3': 'orange'}, A_perturbed_gpt: {'0': 'unknown', '1': 'unknown', '2': 'unknown', '3': 'orange'}\n",
      "Q: \"What color is the cap, stalk, and gills on a Lactarius evosmus?\"\n",
      "A: ['\"The cap, stalk, and gills on a Lactarius evosmus are white.\"']\n",
      "A_llava: A Lactarius evosmus mushroom typically has a white cap, a brown stalk, and white gills.\n",
      "A_llava_blank: A Lactarius evosmus mushroom has a white cap, stalk, and gills.\n",
      "A_perturbed_llava: A Lactarius evosmus has a yellow cap, a white stalk, and white gills.\n",
      "2, A_perturbed: {'0': 'yellow', '1': 'golden', '2': 'yellow', '3': 'yellow'}, A_perturbed_gpt: {'0': 'yellow', '1': 'yellow', '2': 'yellow', '3': 'yellow'}\n",
      "Q: \"Which colors did the torch bearer wear during the 2008 olympic torch relay in London?\"\n",
      "A: ['\"The torch bearer wore red and white clothes during the 2008 Olympic torch relay in London.\"']\n",
      "A_llava: The torch bearer in the image is wearing a red and white outfit while holding the Olympic torch.\n",
      "A_llava_blank: The torch bearer during the 2008 Olympic Torch Relay in London wore a white and red outfit.\n",
      "A_perturbed_llava: The torch bearer in the image is wearing a white hat and a red and yellow jacket.\n",
      "3, A_perturbed: {'0': 'blue', '1': 'blue', '2': 'violet', '3': 'white'}, A_perturbed_gpt: {'1': 'blue', '2': 'blue'}\n",
      "Q: \"What color is the backdrop behind the golden gun displayed at the International Spy Museum?\"\n",
      "A: ['\"The backdrop behind the golden gun displayed at the International Spy Museum is red.\"']\n",
      "A_llava: The backdrop behind the golden gun displayed at the International Spy Museum is red.\n",
      "A_llava_blank: The backdrop behind the golden gun displayed at the International Spy Museum is white.\n",
      "A_perturbed_llava: The backdrop behind the golden gun displayed at the International Spy Museum is green.\n",
      "1, A_perturbed: {'0': 'brown', '1': 'teal'}, A_perturbed_gpt: {'0': 'error code 400 error message invalid content type imageurl be only support by certain model type invalidrequesterror param messages.1content.2type code none', '1': 'teal'}\n",
      "Q: \"What color are the feet of a Bearded Helmetcrest?\"\n",
      "A: ['\"The feet of a Bearded Helmetcrest are black.\"']\n",
      "A_llava: The feet of a Bearded Helmetcrest are black.\n",
      "A_llava_blank: The feet of a Bearded Helmetcrest are black.\n",
      "A_perturbed_llava: The feet of a Bearded Helmetcrest are black.\n",
      "0, A_perturbed: {'0': 'teal', '1': 'golden', '2': 'teal', '3': 'violet'}, A_perturbed_gpt: {'3': 'teal'}\n",
      "Q: \"What is the color of the shoes worn by the man in the sculpture \"Blue Coat Scholar\"?\"\n",
      "A: ['\"Blue shoes are worn by the man in the sculpture \"Blue Coat Scholar\".\"']\n",
      "A_llava: The man in the sculpture \"Blue Coat Scholar\" is wearing yellow shoes.\n",
      "A_llava_blank: The man in the sculpture \"Blue Coat Scholar\" is wearing black shoes.\n",
      "A_perturbed_llava: The man in the sculpture \"Blue Coat Scholar\" is wearing yellow shoes.\n",
      "2, A_perturbed: {'0': 'red', '1': 'red', '2': 'gray'}, A_perturbed_gpt: {'1': 'red'}\n",
      "Q: \"What color can the different species of Amanita mushrooms be?\"\n",
      "A: ['\"Different species of Amanita mushrooms can be white and red.\"']\n",
      "A_llava: Amanita mushrooms can be found in various colors, including red, orange, yellow, and white. The red and white mushroom in the image is an example of one of the many species of Amanita mushrooms. These mushrooms are known for their distinctive appearance and can be found in different habitats, such as forests, grasslands, and even urban areas. It is essential to be cautious when identifying and consuming mushrooms, as some species can be toxic and pose a risk to human health.\n",
      "A_llava_blank: Amanita mushrooms can be found in various colors, depending on the species. Some common colors include red, yellow, orange, brown, and white. However, it is important to note that the color of an Amanita mushroom can vary depending on factors such as the specific species, the environment, and the stage of growth. It is always recommended to consult with an expert or a reliable source when identifying mushrooms to ensure that they are safe to consume.\n",
      "A_perturbed_llava: Amanita mushrooms can be found in various colors, including pink, red, yellow, and white. In the image, there are two pink mushrooms growing on a tree branch, which is a common color for Amanita mushrooms. The different species of Amanita mushrooms can have distinct colors and patterns, which can help in identifying their specific species. However, it is essential to be cautious when identifying mushrooms, as some species can be toxic and pose a risk to human health. It is always advisable to consult with an expert or use a reliable identification guide when dealing with mushrooms.\n",
      "3, A_perturbed: {'0': 'gold', '1': 'violet', '2': 'gray', '3': 'pink'}, A_perturbed_gpt: {'1': 'gold', '4': 'pink'}\n",
      "Q: \"What colors are visible on the signs of both the former Famous Famiglia pizzeria at 757 Broadway, New York and the Sbarro at 7th Avenue and West 47th Street near Times Square, New York?\"\n",
      "A: ['\"Both signs have red, white, and green.\"']\n",
      "A_llava: The colors visible on the signs of the former Famous Famiglia pizzeria at 757 Broadway, New York, and the Sbarro at 7th Avenue and West 47th Street near Times Square, New York, are red and white.\n",
      "A_llava_blank: The signs of the former Famous Famiglia pizzeria at 757 Broadway, New York, and the Sbarro at 7th Avenue and West 47th Street near Times Square, New York, are both white.\n",
      "A_perturbed_llava: The signs of the former Famous Famiglia pizzeria at 757 Broadway, New York, and the Sbarro at 7th Avenue and West 47th Street near Times Square, New York, are both black and white.\n",
      "2, A_perturbed: {'0': 'red', '1': 'black', '2': 'black', '3': 'yellow'}, A_perturbed_gpt: {'1': 'red'}\n",
      "Q: \"What are the colors of the Moscow torch?\"\n",
      "A: ['\"The Moscow torch is red, white, and gold.\"']\n",
      "A_llava: The Moscow torch is gold and white in color.\n",
      "A_llava_blank: The Moscow torch is red and white.\n",
      "A_perturbed_llava: The Moscow torch is gold and black in color.\n",
      "3, A_perturbed: {'0': 'gold', '1': 'blue', '2': 'blue', '3': 'gold'}, A_perturbed_gpt: {'0': 'gold', '1': 'unknown', '2': 'blue', '3': 'gold'}\n",
      "Q: \"What color do most people wear walking around Bin Dawood?\"\n",
      "A: ['\"Most people wear white walking around Bin Dawood.\"']\n",
      "A_llava: Most people walking around Bin Dawood are wearing white.\n",
      "A_llava_blank: Most people in Bin Dawood wear white clothing while walking around.\n",
      "A_perturbed_llava: Most people walking around Bin Dawood are wearing white.\n",
      "1, A_perturbed: {'0': 'yellow', '1': 'golden', '2': 'yellow'}, A_perturbed_gpt: {'1': 'yellow', '3': 'yellow'}\n",
      "Q: \"What colors are on both the The Original Tour bus and the Tower bridge in London?\"\n",
      "A: ['\"The Original Tour bus and the Tower Bridge in London both contain the colors blue, red, yellow and white.\"']\n",
      "A_llava: The The Original Tour bus is red and white, while the Tower Bridge in London is red.\n",
      "A_llava_blank: The Original Tour bus in London is red and white, while the Tower Bridge is a combination of red and white as well.\n",
      "A_perturbed_llava: The Original Tour bus in London is black and white, while the Tower Bridge is also black and white.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'gray', '2': 'orange', '3': 'black'}, A_perturbed_gpt: {'0': 'orange', '1': 'error code 400 error message invalid image url messages1content4imageurlurl expect base64encode datum url with image mime type eg dataimagepngbase64aw1nigj5dgvzighlcmu but get value without data prefix type invalidrequesterror param messages1content4imageurlurl code invalidvalue', '2': 'unknown', '3': 'unknown'}\n",
      "Q: \"What color is the metal work at the stairs of the Grand Palais?\"\n",
      "A: ['\"The metal work at the stairs of the Grand Palais is yellow, green and black.\"']\n",
      "A_llava: The metal work at the stairs of the Grand Palais is yellow.\n",
      "A_llava_blank: The metal work at the stairs of the Grand Palais is black.\n",
      "A_perturbed_llava: The metal work at the stairs of the Grand Palais is red and black.\n",
      "2, A_perturbed: {'0': 'orange', '1': 'gold', '2': 'black', '3': 'gold'}, A_perturbed_gpt: {'1': 'orange', '2': 'gold', '4': 'gold'}\n",
      "Q: \"What colors are the Pabellon Bizantino light up by at night?\"\n",
      "A: ['\"The Pabellon Bizantino is lit up by white and orange lights at night.\"']\n",
      "A_llava: The Pabellon Bizantino is lit up by orange and white lights at night.\n",
      "A_llava_blank: The Pabellon Bizantino is lit up by blue and white lights at night.\n",
      "A_perturbed_llava: The Pabellon Bizantino is lit up by red and white lights at night.\n",
      "2, A_perturbed: {'0': 'pink', '1': 'pink', '2': 'teal', '3': 'violet'}, A_perturbed_gpt: {'1': 'pink', '2': 'pink'}\n",
      "Q: \"What colors were shared between both the hat and the shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018?\"\n",
      "A: ['\"Red, white, and black were shared between both the hat and the shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018.\"']\n",
      "A_llava: The hat and shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018, both had red and white colors.\n",
      "A_llava_blank: The hat and shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018, both had white and black colors.\n",
      "A_perturbed_llava: The hat and shirt worn by Kim A-lang when she threw out the ceremonial first pitch on March 25th, 2018, both had white colors.\n",
      "3, A_perturbed: {'0': 'blue', '1': 'orange', '2': 'gray', '3': 'white'}, A_perturbed_gpt: {'1': 'blue', '4': 'white'}\n",
      "Q: \"What color spots do Paphiopedilum acmodontum have?\"\n",
      "A: ['\"Paphiopedilum acmodontum have dark brown colored spots.\"']\n",
      "A_llava: Paphiopedilum acmodontum has pink spots.\n",
      "A_llava_blank: Paphiopedilum acmodontum has white spots on its leaves.\n",
      "A_perturbed_llava: Paphiopedilum acmodontum has yellow spots.\n",
      "3, A_perturbed: {'0': 'purple', '1': 'brown', '2': 'blue', '3': 'gold'}, A_perturbed_gpt: {'1': 'purple', '2': 'brown', '3': 'blue'}\n",
      "Q: \"What colors are in the imagery of the logo for SeaWorld San Antonio?\"\n",
      "A: ['\"The imagery on the logo for SeaWorld San Antonio is blue and yellow.\"']\n",
      "A_llava: The imagery of the logo for SeaWorld San Antonio features blue and yellow colors.\n",
      "A_llava_blank: The logo for SeaWorld San Antonio features a combination of blue and white colors.\n",
      "A_perturbed_llava: The imagery of the logo for SeaWorld San Antonio features yellow and black colors.\n",
      "2, A_perturbed: {'0': 'yellow', '1': 'purple', '2': 'brown'}, A_perturbed_gpt: {'1': 'yellow'}\n",
      "Q: \"What is the color of the petals and pistil of the flower that can grow on a Astrophytum myriostigma nudum?\"\n",
      "A: ['\"The petals and pistil of the flower that grows on a Astrophytum myriostigma nudum are white and yellow.\"']\n",
      "A_llava: The petals of the flower that can grow on an Astrophytum myriostigma nudum are yellow, and the pistil is white.\n",
      "A_llava_blank: The petals and pistil of the flower that can grow on an Astrophytum myriostigma nudum are white.\n",
      "A_perturbed_llava: The petals of the flower that can grow on an Astrophytum myriostigma nudum are pink, and the pistil is white.\n",
      "2, A_perturbed: {'0': 'pink', '1': 'orange', '2': 'pink', '3': 'black'}, A_perturbed_gpt: {'1': 'pink', '2': 'orange', '3': 'pink'}\n",
      "Q: \"In the marina at pier 39, what color is the pier 39 sign?\"\n",
      "A: ['\"In the marina at Pier 39, the Pier 39 sign is blue and white.\"']\n",
      "A_llava: The pier 39 sign is blue and white.\n",
      "A_llava_blank: The pier 39 sign is white.\n",
      "A_perturbed_llava: The pier 39 sign is blue.\n",
      "0, A_perturbed: {'0': 'pink', '1': 'pink', '2': 'golden', '3': 'pink'}, A_perturbed_gpt: {'2': 'pink'}\n",
      "Q: \"What color is the border around Malevich's Black Square?\"\n",
      "A: ['\"The border around Malevich\\'s Black Square is white.\"']\n",
      "A_llava: The border around Malevich's Black Square is white.\n",
      "A_llava_blank: The border around Malevich's Black Square is black.\n",
      "A_perturbed_llava: The border around Malevich's Black Square is gold.\n",
      "3, A_perturbed: {'0': 'red', '1': 'pink', '2': 'blue', '3': 'golden'}, A_perturbed_gpt: {'0': 'yellow', '1': 'pink', '2': 'gold', '3': 'gold'}\n",
      "Q: \"What colors are the tower on top of the Denpatsu Building, at Ginza, Chuo, Tokyo?\"\n",
      "A: ['\"There is red and white.\"']\n",
      "A_llava: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is red and white.\n",
      "A_llava_blank: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is white and black.\n",
      "A_perturbed_llava: The tower on top of the Denpatsu Building at Ginza, Chuo, Tokyo is blue and white.\n",
      "3, A_perturbed: {'0': 'white', '1': 'golden', '2': 'brown', '3': 'violet'}, A_perturbed_gpt: {'1': 'white', '2': 'golden'}\n",
      "Q: \"What color hair has Hope Solo had throughout her career?\"\n",
      "A: ['\"Hope Solo had blonde and brown colored hair throughout her career.\"']\n",
      "A_llava: Hope Solo has had red hair throughout her career.\n",
      "A_llava_blank: Hope Solo, a former American soccer goalkeeper, has had blonde hair throughout her career.\n",
      "A_perturbed_llava: Hope Solo has had blonde hair throughout her career.\n",
      "3, A_perturbed: {'0': 'blue', '1': 'gray', '2': 'blue', '3': 'blue'}, A_perturbed_gpt: {'1': 'blue'}\n",
      "Q: \"What color are the dishes that the food is served in at Ohana in Belltown?\"\n",
      "A: ['\"The dishes that the food is served in at Ohana in Belltown are white, black and tan.\"']\n",
      "A_llava: The dishes that the food is served in at Ohana in Belltown are black.\n",
      "A_llava_blank: The dishes that the food is served in at Ohana in Belltown are white.\n",
      "A_perturbed_llava: The dishes that the food is served in at Ohana in Belltown are blue and white.\n",
      "2, A_perturbed: {'0': 'red', '1': 'black', '2': 'blue', '3': 'violet'}, A_perturbed_gpt: {'1': 'red', '2': 'black', '3': 'blue', '4': 'violet'}\n",
      "Q: \"What color is the cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul?\"\n",
      "A: ['\"The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\"']\n",
      "A_llava: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "A_llava_blank: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "A_perturbed_llava: The cross on top of the domed part of Cathedral Basilica of Saints Peter and Paul is gold.\n",
      "2, A_perturbed: {'0': 'golden', '1': 'teal', '2': 'teal'}, A_perturbed_gpt: {'1': 'golden'}\n",
      "Q: \"At the Victoria Station, what color tour bus company can you catch?\"\n",
      "A: ['\"You can catch maroon and white company tour buses.\"']\n",
      "A_llava: At the Victoria Station, you can catch a red double-decker tour bus.\n",
      "A_llava_blank: At the Victoria Station, you can catch a white tour bus company.\n",
      "A_perturbed_llava: At the Victoria Station, you can catch a white tour bus. The bus is parked in front of a building, and there are several people walking around the area. The presence of the bus suggests that it is a designated transportation option for tourists or visitors to the area.\n",
      "3, A_perturbed: {'0': 'pink', '1': 'teal', '2': 'brown', '3': 'gray'}, A_perturbed_gpt: {'1': 'pink'}\n",
      "Q: \"What color scheme do the Phallus Mushroom and the Russula ochroleuca share?\"\n",
      "A: ['\"The Phallus Mushroom and the Russula ochroleuca share a yellow and white color scheme.\"']\n",
      "A_llava: The Phallus mushroom and the Russula ochroleuca share a yellow and white color scheme. In the image, the Phallus mushroom is yellow with white spots, while the Russula ochroleuca has a yellow cap with white spots. This color scheme is a common characteristic of these two species, making them easily identifiable in their natural habitats.\n",
      "A_llava_blank: The Phallus mushroom and the Russula ochroleuca share a white color scheme. In the image, both the mushroom and the mushroom-like object have a white color, which is a common characteristic of these two species. The white coloration can be an adaptation to their environment, as it helps them blend in with their surroundings and avoid detection by predators.\n",
      "A_perturbed_llava: The Phallus mushroom and the Russula ochroleuca share a similar color scheme, as both have a brown cap and a white stem. This color combination is common among mushrooms, as it helps them blend in with their natural surroundings, making them less noticeable to potential predators or other animals. The brown cap of the Phallus mushroom and the white stem of the Russula ochroleuca are both distinctive features that help them stand out among other mushrooms in the forest.\n",
      "3, A_perturbed: {'0': 'yellow', '1': 'gold', '2': 'gray', '3': 'brown'}, A_perturbed_gpt: {'1': 'yellow'}\n",
      "Q: \"What colors are seen from the Fountain's statues in the Parc de Versailles ?\"\n",
      "A: ['\"The colors seen from the Fountain\\'s statues in the Parc de Versailles are white and gold.\"']\n",
      "A_llava: The statues in the Parc de Versailles are primarily gold and brown.\n",
      "A_llava_blank: The Fountain's statues in the Parc de Versailles are white and black.\n",
      "A_perturbed_llava: The Fountain's statues in the Parc de Versailles feature pink and gold colors.\n",
      "3, A_perturbed: {'0': 'golden', '1': 'golden', '2': 'black', '3': 'pink'}, A_perturbed_gpt: {'1': 'golden', '2': 'golden', '4': 'pink'}\n",
      "Q: \"What color is the HSBC Hong Kong Headquarters building?\"\n",
      "A: ['\"The HSBC Hong Kong Headquarters building is white.\"']\n",
      "A_llava: The HSBC Hong Kong Headquarters building is white.\n",
      "A_llava_blank: The HSBC Hong Kong Headquarters building is white.\n",
      "A_perturbed_llava: The HSBC Hong Kong Headquarters building is white.\n",
      "2, A_perturbed: {'0': 'orange', '1': 'gray', '2': 'violet'}, A_perturbed_gpt: {'1': 'orange'}\n",
      "Q: \"What color are the letterings of the road signs to the right of the Interstate 80 between Exit 12 and Exit 4 in Blairstown Township, and to the right of Interstate 80 just east of Exit 12 in Hope Township?\"\n",
      "A: ['\"The lettering of the signs to the right of the Interstate 80 between Exit 12 and Exit 4 in Blairstown Township, and to the right of Interstate 80 just east of Exit 12 in Hope Township, are both white.\"']\n",
      "A_llava: The letterings of the road signs to the right of the Interstate 80 between Exit 12 and Exit 4 in Blairstown Township, and to the right of Interstate 80 just east of Exit 12 in Hope Township are white.\n",
      "A_llava_blank: The letterings of the road signs to the right of the Interstate 80 between Exit 12 and Exit 4 in Blairstown Township, and to the right of Interstate 80 just east of Exit 12 in Hope Township are white.\n",
      "A_perturbed_llava: The letterings of the road signs to the right of the Interstate 80 between Exit 12 and Exit 4 in Blairstown Township, and to the right of Interstate 80 just east of Exit 12 in Hope Township are white.\n",
      "1, A_perturbed: {'0': 'teal', '1': 'gray', '2': 'orange', '3': 'gray'}, A_perturbed_gpt: {'0': 'unknown', '1': 'unknown', '2': 'orange', '3': 'unknown'}\n",
      "Q: \"What colors of flames are emitted from the torch on the 1980 USSR Olympic mascot stamp?\"\n",
      "A: ['\"On the 1980 USSR Olympic mascot stamp, the color of the flames emitted from the torch are red, orange, yellow, green, and blue.\"']\n",
      "A_llava: The torch on the 1980 USSR Olympic mascot stamp emits red, white, and blue flames.\n",
      "A_llava_blank: The torch on the 1980 USSR Olympic mascot stamp emits red and blue flames.\n",
      "A_perturbed_llava: The torch on the 1980 USSR Olympic mascot stamp emits purple and yellow flames.\n",
      "3, A_perturbed: {'0': 'orange', '1': 'violet', '2': 'purple', '3': 'violet'}, A_perturbed_gpt: {'0': 'orange', '1': 'purple', '2': 'unknown', '3': 'purple'}\n",
      "Q: \"What color seats are available to sit in at Camp Nou?\"\n",
      "A: ['\"There are red, yellow, blue, and white seats in at Camp Nou.\"']\n",
      "A_llava: At Camp Nou, the seats available to sit in are red.\n",
      "A_llava_blank: At Camp Nou, the seats available to sit in are white.\n",
      "A_perturbed_llava: At Camp Nou, the seats available to sit in are blue.\n",
      "3, A_perturbed: {'0': 'purple', '1': 'pink', '2': 'white', '3': 'golden'}, A_perturbed_gpt: {'1': 'purple', '2': 'pink'}\n",
      "Q: \"What is the color of the Lighthouse, alongside the Surathkal Beach?\"\n",
      "A: ['\"The Lighthouse alongside the Surathkal Beach is red and white.\"']\n",
      "A_llava: The color of the Lighthouse alongside the Surathkal Beach is red and white.\n",
      "A_llava_blank: The color of the lighthouse alongside the Surathkal Beach is white.\n",
      "A_perturbed_llava: The color of the Lighthouse is white.\n",
      "2, A_perturbed: {'0': 'pink', '1': 'pink', '2': 'gray'}, A_perturbed_gpt: {'1': 'pink', '2': 'pink'}\n",
      "Q: \"Which color fireworks were present at the closing ceremonies of both the 2012 and 2016 Summer Olympics?\"\n",
      "A: ['\"There were red and yellow fireworks at the closing ceremonies of both the 2012 and 2016 Summer Olympics.\"']\n",
      "A_llava: The fireworks at the closing ceremonies of both the 2012 and 2016 Summer Olympics were predominantly red, white, and blue. These colors are often associated with patriotism and national pride, and they are commonly used in Olympic ceremonies to represent the host country. The fireworks display in the image is a reflection of this tradition, as the red, white, and blue fireworks create a vibrant and memorable spectacle for the audience.\n",
      "A_llava_blank: The closing ceremonies of both the 2012 and 2016 Summer Olympics featured a fireworks display in the colors of the Union Jack, which is the national flag of the United Kingdom. The Union Jack consists of three horizontal crosses representing the colors of the British flag: red (representing England), white (representing Scotland), and blue (representing Ireland, now Northern Ireland). The fireworks display at the closing ceremonies of both the 2012 and 2016 Olympics showcased these colors, symbolizing the United Kingdom's role in hosting the events.\n",
      "A_perturbed_llava: The closing ceremonies of both the 2012 and 2016 Summer Olympics featured fireworks in the colors of red, white, and blue. These colors are often associated with patriotism and national pride, and they are commonly used in Olympic ceremonies to represent the host country. The fireworks display in the image is a part of the closing ceremony for the 2016 Summer Olympics, held in Rio de Janeiro, Brazil.\n",
      "3, A_perturbed: {'0': 'pink', '1': 'yellow', '2': 'yellow', '3': 'yellow'}, A_perturbed_gpt: {'2': 'yellow', '4': 'yellow'}\n",
      "Q: \"What color is the world cup trophy?\"\n",
      "A: ['\"The World Cup Trophy is gold with a green base.\"']\n",
      "A_llava: The world cup trophy is gold.\n",
      "A_llava_blank: The world cup trophy is gold.\n",
      "A_perturbed_llava: The world cup trophy is gold.\n",
      "3, A_perturbed: {'0': 'golden', '1': 'pink', '2': 'yellow', '3': 'golden'}, A_perturbed_gpt: {'1': 'golden', '4': 'golden'}\n",
      "Q: \"What colors are the stadium seats at Camp Nou in Barcelona?\"\n",
      "A: ['\"The stadium seats at Camp Nou in Barcelona are blue and red.\"']\n",
      "A_llava: The stadium seats at Camp Nou in Barcelona are predominantly red and yellow.\n",
      "A_llava_blank: The stadium seats at Camp Nou in Barcelona are predominantly blue and white.\n",
      "A_perturbed_llava: The stadium seats at Camp Nou in Barcelona are blue and yellow.\n",
      "3, A_perturbed: {'0': 'blue', '1': 'gray', '2': 'gray', '3': 'gold'}, A_perturbed_gpt: {'1': 'blue'}\n",
      "Q: \"What color are some of the walls at the Van Gogh Museum?\"\n",
      "A: ['\"Red, white, yellow, and brown are the colors on some of the walls at the Van Gogh Museum.\"']\n",
      "A_llava: The walls at the Van Gogh Museum are painted yellow.\n",
      "A_llava_blank: The walls at the Van Gogh Museum are painted white.\n",
      "A_perturbed_llava: The walls at the Van Gogh Museum are painted white.\n",
      "3, A_perturbed: {'0': 'gray', '1': 'teal', '2': 'violet', '3': 'black'}, A_perturbed_gpt: {'1': 'gray', '3': 'violet'}\n",
      "Q: \"What color are the top tips of the wings on a Dysphania percota?\"\n",
      "A: ['\"The top tips of the wings on a Dysphania percota are black.\"']\n",
      "A_llava: The top tips of the wings on a Dysphania percota are blue.\n",
      "A_llava_blank: The top tips of the wings on a Dysphania percota are black.\n",
      "A_perturbed_llava: The top tips of the wings on a Dysphania percota are black.\n",
      "0, A_perturbed: {'0': 'white', '1': 'yellow', '2': 'purple'}, A_perturbed_gpt: {'0': 'black', '1': 'yellow', '2': 'purple'}\n",
      "Q: \"What is the color of the paint worn on the face by the man in George Catlin's \"Eeh-tow-wées-ka-zeet, He Who Has Eyes Behind Him (also known as Broken Arm), a Foremost Brave\" and the woman in \"Eeh-nís-kim, Crystal Stone, Wife of the Chief\" also by George Catlin?\"\n",
      "A: ['\"The paint worn on the face by the man in George Catlin\\'s \"Eeh-tow-wées-ka-zeet, He Who Has Eyes Behind Him (also known as Broken Arm), a Foremost Brave\" and the woman in \"Eeh-nís-kim, Crystal Stone, Wife of the Chief\" also by George Catlin is red.\"']\n",
      "A_llava: The color of the paint worn on the face by the man in George Catlin's \"Eeh-tow-wées-ka-zeet, He Who Has Eyes Behind Him (also known as Broken Arm), a Foremost Brave\" is red. The woman in \"Eeh-nís-kim, Crystal Stone, Wife of the Chief\" also has red paint on her face.\n",
      "A_llava_blank: The color of the paint worn on the face by the man in George Catlin's \"Eeh-tow-wées-ka-zeet, He Who Has Eyes Behind Him (also known as Broken Arm), a Foremost Brave\" and the woman in \"Eeh-nís-kim, Crystal Stone, Wife of the Chief\" is red.\n",
      "A_perturbed_llava: The color of the paint worn on the face by the man in George Catlin's \"Eeh-tow-wées-ka-zeet, He Who Has Eyes Behind Him (also known as Broken Arm), a Foremost Brave\" is red. The woman in \"Eeh-nís-kim, Crystal Stone, Wife of the Chief\" also has red paint on her face.\n",
      "2, A_perturbed: {'0': 'purple', '1': 'gold', '2': 'white'}, A_perturbed_gpt: {'1': 'purple'}\n",
      "Q: \"What color is the belly of a Green Tree Frog?\"\n",
      "A: ['\"The belly of a Green Tree Frog is white.\"']\n",
      "A_llava: The belly of a Green Tree Frog is white.\n",
      "A_llava_blank: The belly of a Green Tree Frog is white.\n",
      "A_perturbed_llava: The belly of a Green Tree Frog is green.\n",
      "3, A_perturbed: {'0': 'pink', '1': 'violet', '2': 'purple', '3': 'yellow'}, A_perturbed_gpt: {'1': 'pink'}\n",
      "Q: \"What colored spots does the Dinocampus_coccinellae_cocon have on its shell?\"\n",
      "A: ['\"The Dinocampus coccinellae cocon has black spots on its shell.\"']\n",
      "A_llava: The Dinocampus coccinellae cocon has red spots on its shell.\n",
      "A_llava_blank: The Dinocampus coccinellae cocon has red spots on its shell.\n",
      "A_perturbed_llava: The Dinocampus coccinellae cocon has black spots on its shell.\n",
      "1, A_perturbed: {'0': 'purple', '1': 'white'}, A_perturbed_gpt: {'1': 'purple'}\n"
     ]
    }
   ],
   "source": [
    "for key in parametric_ans_keys:\n",
    "    print(f\"Q: {eval_data[key]['Q']}\")\n",
    "    print(f\"A: {eval_data[key]['A']}\")\n",
    "    print(f\"A_llava: {eval_data[key]['A_llava']}\")\n",
    "    print(f\"A_llava_blank: {eval_data[key]['A_llava_blank']}\")\n",
    "    idx = perturbation_ignored[key]\n",
    "    print(f\"A_perturbed_llava: {eval_data[key]['A_perturbed_llava'][idx]}\")\n",
    "    \n",
    "    print(f\"{idx}, A_perturbed: {eval_data[key]['A_perturbed']}, A_perturbed_gpt: {eval_data[key]['A_perturbed_gpt']}\")\n",
    "    # , A_llava_blank: {eval_data[key]['A_llava_blank']}, A_perturbed_llava: {eval_data[key]['A_perturbed_llava']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
